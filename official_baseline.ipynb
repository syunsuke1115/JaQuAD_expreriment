{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自分で書いたオリジナルのベースラインモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "from typing import Any, Dict, Iterator, List, Tuple, Union\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import transformers \n",
    "from transformers import BertJapaneseTokenizer, ElectraForMaskedLM,ElectraForQuestionAnswering\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning import loggers\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AdamW\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'random_seed': 42,  # Random Seed\n",
    "    # Transformers PLM name.\n",
    "    'pretrained_model': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "    # Optional, Transformers Tokenizer name. Overrides `pretrained_model`\n",
    "    'pretrained_tokenizer': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "    'norm_form': 'NFKC',\n",
    "    'batch_size': 8,  # <=32 for TPUv2-8\n",
    "    'lr': 2e-5,  # Learning Rate\n",
    "    'max_length': 384,  # Max Length input size\n",
    "    'doc_stride': 128,  # The interval of the context when splitting is needed\n",
    "    'epochs': 3,  # Max Epochs\n",
    "    'dataset': 'SkelterLabsInc/JaQuAD',\n",
    "    'huggingface_auth_token': None,\n",
    "    'test_mode': False,  # Test Mode enables `fast_dev_run`\n",
    "    'optimizer': 'AdamW',\n",
    "    'weight_decay': 0.01,  # Weight decaying parameter for AdamW\n",
    "    'lr_scheduler': 'warmup_lin',\n",
    "    'warmup_ratio': 0.1,\n",
    "    'fp16': False,  # Enable train on FP16 (if GPU)\n",
    "    'tpu_cores': 8,  # Enable TPU with 1 core or 8 cores\n",
    "    'cpu_workers': os.cpu_count(), #questionとcontextを合わせた最大語数。今回のデータセットは1300は超えない\n",
    "    'note':\"リクルートベースライン\",\n",
    "}\n",
    "\n",
    "args\n",
    "\n",
    "#seed値を固定\n",
    "def set_seed(seed =42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "set_seed(seed=args[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "import wandb\n",
    "wandb.login\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"JaQuad\",config=args,\n",
    "                 name = args[\"note\"] + \"_\"+args[\"pretrained_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ja_qu_ad (/home/s16991/.cache/huggingface/datasets/SkelterLabsInc___ja_qu_ad/default/0.1.0/5847b2e2ab5e02de284395bb15f87f13eae8f6f6ff1f01e4ee9c5c0dcf8ef8eb)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f999779a6534e9e96fa1e9f0e52361c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetdict = datasets.load_dataset(\n",
    "    args['dataset'], use_auth_token=args['huggingface_auth_token'])\n",
    "datasetdict = datasetdict.flatten()\\\n",
    "            .rename_column('answers.text', 'answer')\\\n",
    "            .rename_column('answers.answer_start', 'answer_start')\\\n",
    "            .rename_column('answers.answer_type', 'answer_type')\n",
    "\n",
    "# train =pd.DataFrame(jaquad_dataset['train'][:].values(), index=jaquad_dataset['train'][:].keys()).T\n",
    "# valid =pd.DataFrame(jaquad_dataset['validation'][:].values(), index=jaquad_dataset['validation'][:].keys()).T\n",
    "# train.to_csv(\"JaQuAD_train.csv\")\n",
    "# valid.to_csv(\"JaQuAD_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples['question'],\n",
    "            examples['context'],\n",
    "        )\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': [],\n",
    "            'attention_mask': [],\n",
    "            'token_type_ids': [],\n",
    "            'start_positions': [],\n",
    "            'end_positions': [],\n",
    "        }\n",
    "        for tokens, att_mask, type_ids, context, answer, start_char \\\n",
    "                in zip(tokenized_examples['input_ids'],\n",
    "                       tokenized_examples['attention_mask'],\n",
    "                       tokenized_examples['token_type_ids'],\n",
    "                       examples['context'],\n",
    "                       examples['answer'],\n",
    "                       examples['answer_start']):\n",
    "            answer = answer[0]\n",
    "            start_char = start_char[0]\n",
    "            offsets = get_offsets(tokens, context, tokenizer,\n",
    "                                  args[\"norm_form\"])\n",
    "\n",
    "            ctx_start = tokens.index(tokenizer.sep_token_id) + 1\n",
    "            answer_start_index = ctx_start\n",
    "            answer_end_index = len(offsets) - 1\n",
    "            while offsets[answer_start_index][0] < start_char:\n",
    "                answer_start_index += 1\n",
    "            while offsets[answer_end_index][1] > start_char + len(answer):\n",
    "                answer_end_index -= 1\n",
    "\n",
    "            span_inputs = {\n",
    "                'input_ids': tokens,\n",
    "                'attention_mask': att_mask,\n",
    "                'token_type_ids': type_ids,\n",
    "            }\n",
    "            for span, answer_idx in make_spans(\n",
    "                    span_inputs,\n",
    "                    question_len=ctx_start,\n",
    "                    max_seq_len=args[\"max_length\"],\n",
    "                    stride=args[\"doc_stride\"],\n",
    "                    answer_start_position=answer_start_index,\n",
    "                    answer_end_position=answer_end_index):\n",
    "                inputs['input_ids'].append(span['input_ids'])\n",
    "                inputs['attention_mask'].append(span['attention_mask'])\n",
    "                inputs['token_type_ids'].append(span['token_type_ids'])\n",
    "                inputs['start_positions'].append(answer_idx[0])\n",
    "                inputs['end_positions'].append(answer_idx[1])\n",
    "        print(len(inputs))\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def make_spans(\n",
    "    inputs: Dict[str, Union[int, List[int]]],\n",
    "    question_len: int,\n",
    "    max_seq_len: int,\n",
    "    stride: int,\n",
    "    answer_start_position: int = -1,\n",
    "    answer_end_position: int = -1\n",
    ") -> Iterator[Tuple[Dict[str, List[int]], Tuple[int, int]]]:\n",
    "    input_len = len(inputs['input_ids'])\n",
    "    context_len = input_len - question_len\n",
    "\n",
    "    def make_value(input_list, i, padding=0):\n",
    "        context_end = min(max_seq_len - question_len, context_len - i)\n",
    "        pad_len = max_seq_len - question_len - context_end\n",
    "        val = input_list[:question_len]\n",
    "        val += input_list[question_len + i:question_len + i + context_end]\n",
    "        val[-1] = input_list[-1]\n",
    "        val += [padding] * pad_len\n",
    "        return val\n",
    "    for i in range(0, input_len - max_seq_len + stride, stride):\n",
    "        span = {key: make_value(val, i) for key, val in inputs.items()}\n",
    "        answer_start = answer_start_position - i\n",
    "        answer_end = answer_end_position - i\n",
    "        if answer_start < question_len or answer_end >= max_seq_len - 1:\n",
    "            answer_start = answer_end = 0\n",
    "        yield span, (answer_start, answer_end)\n",
    "        \n",
    "\n",
    "def get_offsets(input_ids: List[int],\n",
    "                context: str,\n",
    "                tokenizer: AutoTokenizer,\n",
    "                norm_form='NFKC') -> List[Tuple[int, int]]:\n",
    "    \n",
    "    cxt_start = input_ids.index(tokenizer.sep_token_id) + 1\n",
    "    cxt_end = cxt_start + input_ids[cxt_start:].index(tokenizer.sep_token_id)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[cxt_start:cxt_end])\n",
    "    tokens = [tok[2:] if tok.startswith('##') else tok for tok in tokens]\n",
    "    whitespace = string.whitespace + '\\u3000'\n",
    "\n",
    "    # 1 . Make offsets of normalized context within the original context.\n",
    "    offsets_norm_context = []\n",
    "    norm_context = ''\n",
    "    for idx, char in enumerate(context):\n",
    "        norm_char = unicodedata.normalize(norm_form, char)\n",
    "        norm_context += norm_char\n",
    "        offsets_norm_context.extend([idx] * len(norm_char))\n",
    "    norm_context_org = unicodedata.normalize(norm_form, context)\n",
    "    assert norm_context == norm_context_org, \\\n",
    "        'Normalized contexts are not the same: ' \\\n",
    "        + f'{norm_context} != {norm_context_org}'\n",
    "    assert len(norm_context) == len(offsets_norm_context), \\\n",
    "        'Normalized contexts have different numbers of tokens: ' \\\n",
    "        + f'{len(norm_context)} != {len(offsets_norm_context)}'\n",
    "\n",
    "    # 2. Make offsets of tokens (input_ids) within the normalized context.\n",
    "    offsets_token = []\n",
    "    unk_pointer = None\n",
    "    cid = 0\n",
    "    tid = 0\n",
    "    while tid < len(tokens):\n",
    "        cur_token = tokens[tid]\n",
    "        if cur_token == tokenizer.unk_token:\n",
    "            unk_pointer = tid\n",
    "            offsets_token.append([cid, cid])\n",
    "            cid += 1\n",
    "        elif norm_context[cid:cid + len(cur_token)] != cur_token:\n",
    "            # Wrong offsets of the previous UNK token\n",
    "            assert unk_pointer is not None, \\\n",
    "                'Normalized context and tokens are not matched'\n",
    "            prev_unk_expected = offsets_token[unk_pointer]\n",
    "            prev_unk_expected[1] += norm_context[prev_unk_expected[1] + 2:]\\\n",
    "                .index(tokens[unk_pointer + 1]) + 1\n",
    "            tid = unk_pointer\n",
    "            offsets_token = offsets_token[:tid] + [prev_unk_expected]\n",
    "            cid = prev_unk_expected[1] + 1\n",
    "        else:\n",
    "            start_pos = norm_context[cid:].index(cur_token)\n",
    "            if start_pos > 0 and tokens[tid - 1] == tokenizer.unk_token:\n",
    "                offsets_token[-1][1] += start_pos\n",
    "                cid += start_pos\n",
    "                start_pos = 0\n",
    "            assert start_pos == 0, f'{start_pos} != 0 (cur: {cur_token}'\n",
    "            offsets_token.append([cid, cid + len(cur_token) - 1])\n",
    "            cid += len(cur_token)\n",
    "            while cid < len(norm_context) and norm_context[cid] in whitespace:\n",
    "                offsets_token[-1][1] += 1\n",
    "                cid += 1\n",
    "        tid += 1\n",
    "    if tokens[-1] == tokenizer.unk_token:\n",
    "        offsets_token[-1][1] = len(norm_context) - 1\n",
    "    else:\n",
    "        assert cid == len(norm_context) == offsets_token[-1][1] + 1, \\\n",
    "            'Offsets do not include all characters'\n",
    "    assert len(offsets_token) == len(tokens), \\\n",
    "        'The numbers of tokens and offsets are different'\n",
    "\n",
    "    offsets_mapping = [(offsets_norm_context[start], offsets_norm_context[end])\n",
    "                       for start, end in offsets_token]\n",
    "    return [(-1, -1)] * cxt_start + offsets_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "            args[\"pretrained_tokenizer\"]\n",
    "            if args[\"pretrained_tokenizer\"] else\n",
    "            args[\"pretrained_model\"],\n",
    "            use_auth_token=args[\"huggingface_auth_token\"])\n",
    "tokenized_dataset = datasetdict.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=datasetdict['train'].column_names)\n",
    "#distillbertの際使う\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "            remove_columns=['token_type_ids'])\n",
    "tokenized_dataset.set_format(type='torch')\n",
    "print(len(tokenized_dataset[\"train\"]))\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"],shuffle = True,batch_size=args[\"batch_size\"],num_workers=args[\"cpu_workers\"])\n",
    "valid_dataloader = DataLoader(tokenized_dataset['validation'], shuffle=False,batch_size=args[\"batch_size\"],num_workers=args[\"cpu_workers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLinearLR(lr_scheduler.LambdaLR):\n",
    "    '''The learning rate is linearly increased for the first `warmup_steps`\n",
    "    and linearly decreased to zero afterward.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, optimizer, warmup_steps, max_steps, last_epoch=-1):\n",
    "\n",
    "        def lr_lambda(step):\n",
    "            if step < warmup_steps:\n",
    "                return float(step) / float(max(1.0, warmup_steps))\n",
    "            ratio = 1 - float(step - warmup_steps) / float(max_steps -\n",
    "                                                           warmup_steps)\n",
    "            return max(0.0, min(1.0, ratio))\n",
    "\n",
    "        super().__init__(optimizer, lr_lambda, last_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bertを使ったQAモデル\n",
    "class QAModel(LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # kwargs are saved in self.hparams\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.question_answerer = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            self.hparams.pretrained_model,\n",
    "            use_auth_token=self.hparams.huggingface_auth_token)\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "            self.hparams.pretrained_tokenizer\n",
    "            if self.hparams.pretrained_tokenizer else\n",
    "            self.hparams.pretrained_model,\n",
    "            use_auth_token=self.hparams.huggingface_auth_token)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        return self.question_answerer(**kwargs)\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        print(outputs)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        start_preds = outputs.start_logits.argmax(dim=-1).cpu().detach()\n",
    "        end_preds = outputs.end_logits.argmax(dim=-1).cpu().detach()\n",
    "        start_positions = batch['start_positions'].cpu().detach()\n",
    "        end_positions = batch['end_positions'].cpu().detach()\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'start_preds': start_preds,\n",
    "            'end_preds': end_preds,\n",
    "            'start_positions': start_positions,\n",
    "            'end_positions': end_positions,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt = self.optimizers()\n",
    "        opt.zero_grad()\n",
    "        outputs = self.step(batch, batch_idx)\n",
    "        self.manual_backward(outputs['loss'])\n",
    "        opt.step()\n",
    "\n",
    "        # single scheduler\n",
    "        sch = self.lr_schedulers()\n",
    "        sch.step()\n",
    "        return outputs\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_f1_score(start_positions, end_positions, start_preds,\n",
    "                           end_preds):\n",
    "        start_overlap = np.maximum(start_positions, start_preds)\n",
    "        end_overlap = np.minimum(end_positions, end_preds)\n",
    "        overlap = np.maximum(end_overlap - start_overlap + 1, 0)\n",
    "\n",
    "        pred_token_count = np.maximum(end_preds - start_preds + 1, 0)\n",
    "        ground_token_count = np.maximum(end_positions - start_positions + 1, 0)\n",
    "\n",
    "        precision = torch.nan_to_num(overlap / pred_token_count, nan=0.)\n",
    "        recall = torch.nan_to_num(overlap / ground_token_count, nan=0.)\n",
    "        f1 = torch.nan_to_num(\n",
    "            2 * precision * recall / (precision + recall), nan=0.)\n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_exact_match(start_positions, end_positions, start_preds,\n",
    "                              end_preds):\n",
    "        equal_start = (start_preds == start_positions)\n",
    "        equal_end = (end_preds == end_positions)\n",
    "        return (equal_start * equal_end).type(torch.float)\n",
    "\n",
    "    def epoch_end(self, outputs, state='train'):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        precision = torch.tensor(0, dtype=torch.float)\n",
    "        recall = torch.tensor(0, dtype=torch.float)\n",
    "        f1 = torch.tensor(0, dtype=torch.float)\n",
    "        em = torch.tensor(0, dtype=torch.float)\n",
    "\n",
    "        for i in outputs:\n",
    "            loss += i['loss'].cpu().detach()\n",
    "            f1_metrics = self.calculate_f1_score(i['start_positions'],\n",
    "                                                 i['end_positions'],\n",
    "                                                 i['start_preds'],\n",
    "                                                 i['end_preds'])\n",
    "            precision += f1_metrics['precision'].mean()\n",
    "            recall += f1_metrics['recall'].mean()\n",
    "            f1 += f1_metrics['f1'].mean()\n",
    "            em += self.calculate_exact_match(i['start_positions'],\n",
    "                                             i['end_positions'],\n",
    "                                             i['start_preds'],\n",
    "                                             i['end_preds']).mean()\n",
    "        loss = loss / len(outputs)\n",
    "        precision = precision / len(outputs)\n",
    "        recall = recall / len(outputs)\n",
    "        f1 = f1 / len(outputs)\n",
    "        em = em / len(outputs)\n",
    "        metrics = {\n",
    "            state + '_loss': float(loss),\n",
    "            state + '_precision': precision,\n",
    "            state + '_recall': recall,\n",
    "            state + '_f1': f1,\n",
    "            state + '_em': em,\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, on_epoch=True)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state='train')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state='val')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(\n",
    "                self.parameters(),\n",
    "                lr=self.hparams.lr,\n",
    "                weight_decay=self.hparams.weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError('Only AdamW is Supported!')\n",
    "\n",
    "        if self.hparams.lr_scheduler == 'cos':\n",
    "            scheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=1, T_mult=2)\n",
    "        elif self.hparams.lr_scheduler == 'exp':\n",
    "            scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "        elif self.hparams.lr_scheduler == 'warmup_lin':\n",
    "            steps_per_epoch = len(self.train_dataloader())\n",
    "            if os.environ.get('COLAB_TPU_ADDR'):\n",
    "                steps_per_epoch = steps_per_epoch // self.hparams.tpu_cores\n",
    "            total_steps = steps_per_epoch * self.hparams.epochs\n",
    "            warmup_steps = int(total_steps * self.hparams.warmup_ratio)\n",
    "            scheduler = WarmupLinearLR(\n",
    "                optimizer, warmup_steps=warmup_steps, max_steps=total_steps)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'Only cos, exp, and warmup_lin lr scheduler is Supported!')\n",
    "        return [optimizer], [scheduler]\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filename='val_loss{val_loss:.4f}-val_f1{val_f1:.4f}-epoch{epoch}',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=5,\n",
    "    auto_insert_metric_name=False,\n",
    ")\n",
    "lr_callback = callbacks.LearningRateMonitor(logging_interval='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using PyTorch Ver', torch.__version__)\n",
    "print('Fix Seed:', args['random_seed'])\n",
    "seed_everything(args['random_seed'])\n",
    "\n",
    "model = QAModel(**args)\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[lr_callback,\n",
    "               checkpoint_callback],\n",
    "    log_every_n_steps=16,  # Logging frequency of **learning rate**\n",
    "    max_epochs=args['epochs'],\n",
    "    fast_dev_run=args['test_mode'],\n",
    "    # logger=wandb_logger,\n",
    "    num_sanity_val_steps=None if args['test_mode'] else 0,\n",
    "    # For GPU Setup\n",
    "    deterministic=torch.cuda.is_available(),\n",
    "    gpus=[0] if torch.cuda.is_available() else None,  # Use one GPU (idx 0)\n",
    "    precision=16 if args['fp16'] and torch.cuda.is_available() else 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##　軽量化\n",
    "def quantize_transform(model):\n",
    "    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    return model\n",
    "quantize_transform(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(':: Start Training ::')\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    wandb.login\n",
    "    wandb_logger = WandbLogger(project=\"JaQuad\",config=args,\n",
    "                    name = args[\"note\"] + \"_\"+args[\"pretrained_model\"])\n",
    "    datasetdict = datasets.load_dataset(\n",
    "        args['dataset'], use_auth_token=args['huggingface_auth_token'])\n",
    "    datasetdict = datasetdict.flatten()\\\n",
    "                .rename_column('answers.text', 'answer')\\\n",
    "                .rename_column('answers.answer_start', 'answer_start')\\\n",
    "                .rename_column('answers.answer_type', 'answer_type')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "                args[\"pretrained_tokenizer\"]\n",
    "                if args[\"pretrained_tokenizer\"] else\n",
    "                args[\"pretrained_model\"],\n",
    "                use_auth_token=args[\"huggingface_auth_token\"])\n",
    "    tokenized_dataset = datasetdict.map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                remove_columns=datasetdict['train'].column_names)\n",
    "    #distillbertの際使う\n",
    "    tokenized_dataset = tokenized_dataset.map(remove_columns=['token_type_ids'])\n",
    "    tokenized_dataset.set_format(type='torch')\n",
    "    train_dataloader = DataLoader(tokenized_dataset[\"train\"],shuffle = True,batch_size=args[\"batch_size\"],num_workers=args[\"cpu_workers\"])\n",
    "    valid_dataloader = DataLoader(tokenized_dataset['validation'], shuffle=False,batch_size=args[\"batch_size\"],num_workers=args[\"cpu_workers\"])\n",
    "    checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filename='val_loss{val_loss:.4f}-val_f1{val_f1:.4f}-epoch{epoch}',\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_top_k=5,\n",
    "        auto_insert_metric_name=False,\n",
    "    )\n",
    "    lr_callback = callbacks.LearningRateMonitor(logging_interval='step')\n",
    "    model = QAModel(**args)\n",
    "    trainer = Trainer(\n",
    "        callbacks=[lr_callback,\n",
    "                checkpoint_callback],\n",
    "        log_every_n_steps=16,  # Logging frequency of **learning rate**\n",
    "        max_epochs=args['epochs'],\n",
    "        fast_dev_run=args['test_mode'],\n",
    "        logger=wandb_logger,\n",
    "        num_sanity_val_steps=None if args['test_mode'] else 0,\n",
    "        # For GPU Setup\n",
    "        deterministic=torch.cuda.is_available(),\n",
    "        gpus=[0] if torch.cuda.is_available() else None,  # Use one GPU (idx 0)\n",
    "        precision=16 if args['fp16'] and torch.cuda.is_available() else 32,\n",
    "    )\n",
    "    quantize_transform(model)\n",
    "    trainer.fit(model)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['pretrained_model'] = 'Cinnamon/electra-small-japanese-generator'\n",
    "args['pretrained_tokenizer'] = 'Cinnamon/electra-small-japanese-generator'\n",
    "experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['pretrained_model'] = 'SkelterLabsInc/bert-base-japanese-jaquad'\n",
    "model = QAModel(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ゴミ箱"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
