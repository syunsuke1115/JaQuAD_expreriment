{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEvslqjDCiUM"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "533xQ1HdZBhv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Make sure to select TPU from Edit > Notebook settings > Hardware accelerator\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.environ.get('COLAB_TPU_ADDR'):\n",
        "    print('Make sure to select TPU from Edit > Notebook settings > Hardware accelerator')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65cwlByIqbVC"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GVNY-Mt3rSYn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "import unicodedata\n",
        "from typing import Any, Dict, Iterator, List, Tuple, Union\n",
        "\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmT8NWJbrVFa",
        "outputId": "989e2fd5-8b6c-4cb8-cb76-d602e1a156c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'random_seed': 42,\n",
              " 'pretrained_model': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
              " 'pretrained_tokenizer': '',\n",
              " 'norm_form': 'NFKC',\n",
              " 'batch_size': 32,\n",
              " 'lr': 2e-05,\n",
              " 'max_length': 384,\n",
              " 'doc_stride': 128,\n",
              " 'epochs': 4,\n",
              " 'dataset': 'SkelterLabsInc/JaQuAD',\n",
              " 'huggingface_auth_token': None,\n",
              " 'test_mode': False,\n",
              " 'optimizer': 'AdamW',\n",
              " 'weight_decay': 0.01,\n",
              " 'lr_scheduler': 'warmup_lin',\n",
              " 'warmup_ratio': 0.1,\n",
              " 'fp16': False,\n",
              " 'tpu_cores': 8,\n",
              " 'cpu_workers': 4}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args = {\n",
        "    'random_seed': 42,  # Random Seed\n",
        "    # Transformers PLM name.\n",
        "    'pretrained_model': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
        "    # Optional, Transformers Tokenizer name. Overrides `pretrained_model`\n",
        "    'pretrained_tokenizer': '',\n",
        "    'norm_form': 'NFKC',\n",
        "    'batch_size': 32,  # <=32 for TPUv2-8\n",
        "    'lr': 2e-5,  # Learning Rate\n",
        "    'max_length': 384,  # Max Length input size\n",
        "    'doc_stride': 128,  # The interval of the context when splitting is needed\n",
        "    'epochs': 4,  # Max Epochs\n",
        "    'dataset': 'SkelterLabsInc/JaQuAD',\n",
        "    'huggingface_auth_token': None,\n",
        "    'test_mode': False,  # Test Mode enables `fast_dev_run`\n",
        "    'optimizer': 'AdamW',\n",
        "    'weight_decay': 0.01,  # Weight decaying parameter for AdamW\n",
        "    'lr_scheduler': 'warmup_lin',\n",
        "    'warmup_ratio': 0.1,\n",
        "    'fp16': False,  # Enable train on FP16 (if GPU)\n",
        "    'tpu_cores': 8,  # Enable TPU with 1 core or 8 cores\n",
        "    'cpu_workers': os.cpu_count(),\n",
        "}\n",
        "\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YYsBE7rf0Fm6"
      },
      "outputs": [],
      "source": [
        "class WarmupLinearLR(lr_scheduler.LambdaLR):\n",
        "    '''The learning rate is linearly increased for the first `warmup_steps`\n",
        "    and linearly decreased to zero afterward.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, optimizer, warmup_steps, max_steps, last_epoch=-1):\n",
        "\n",
        "        def lr_lambda(step):\n",
        "            if step < warmup_steps:\n",
        "                return float(step) / float(max(1.0, warmup_steps))\n",
        "            ratio = 1 - float(step - warmup_steps) / float(max_steps -\n",
        "                                                           warmup_steps)\n",
        "            return max(0.0, min(1.0, ratio))\n",
        "\n",
        "        super().__init__(optimizer, lr_lambda, last_epoch=last_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ke6TdX1EX26I"
      },
      "outputs": [],
      "source": [
        "def make_spans(\n",
        "    inputs: Dict[str, Union[int, List[int]]],\n",
        "    question_len: int,\n",
        "    max_seq_len: int,\n",
        "    stride: int,\n",
        "    answer_start_position: int = -1,\n",
        "    answer_end_position: int = -1\n",
        ") -> Iterator[Tuple[Dict[str, List[int]], Tuple[int, int]]]:\n",
        "    input_len = len(inputs['input_ids'])\n",
        "    context_len = input_len - question_len\n",
        "\n",
        "    def make_value(input_list, i, padding=0):\n",
        "        context_end = min(max_seq_len - question_len, context_len - i)\n",
        "        pad_len = max_seq_len - question_len - context_end\n",
        "        val = input_list[:question_len]\n",
        "        val += input_list[question_len + i:question_len + i + context_end]\n",
        "        val[-1] = input_list[-1]\n",
        "        val += [padding] * pad_len\n",
        "        return val\n",
        "\n",
        "    for i in range(0, input_len - max_seq_len + stride, stride):\n",
        "        span = {key: make_value(val, i) for key, val in inputs.items()}\n",
        "        answer_start = answer_start_position - i\n",
        "        answer_end = answer_end_position - i\n",
        "        if answer_start < question_len or answer_end >= max_seq_len - 1:\n",
        "            answer_start = answer_end = 0\n",
        "        yield span, (answer_start, answer_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_offsets(input_ids: List[int],\n",
        "                context: str,\n",
        "                tokenizer: AutoTokenizer,\n",
        "                norm_form='NFKC') -> List[Tuple[int, int]]:\n",
        "    '''The character-level start/end offsets of a token within a context.\n",
        "    Algorithm:\n",
        "    1. Make offsets of normalized context within the original context.\n",
        "    2. Make offsets of tokens (input_ids) within the normalized context.\n",
        "\n",
        "    Arguments:\n",
        "    input_ids -- Token ids of tokenized context (by tokenizer).\n",
        "    context -- String of context\n",
        "    tokenizer\n",
        "    norm_form\n",
        "\n",
        "    Return:\n",
        "        List[Tuple[int, int]]: Offsets of tokens within the input context.\n",
        "        For each token, the offsets are presented as a tuple of (start\n",
        "        position index, end position index). Both indices are inclusive.\n",
        "    '''\n",
        "    cxt_start = input_ids.index(tokenizer.sep_token_id) + 1\n",
        "    cxt_end = cxt_start + input_ids[cxt_start:].index(tokenizer.sep_token_id)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[cxt_start:cxt_end])\n",
        "    tokens = [tok[2:] if tok.startswith('##') else tok for tok in tokens]\n",
        "    whitespace = string.whitespace + '\\u3000'\n",
        "\n",
        "    # 1. Make offsets of normalized context within the original context.\n",
        "    offsets_norm_context = []\n",
        "    norm_context = ''\n",
        "    for idx, char in enumerate(context):\n",
        "        norm_char = unicodedata.normalize(norm_form, char)\n",
        "        norm_context += norm_char\n",
        "        offsets_norm_context.extend([idx] * len(norm_char))\n",
        "    norm_context_org = unicodedata.normalize(norm_form, context)\n",
        "    assert norm_context == norm_context_org, \\\n",
        "        'Normalized contexts are not the same: ' \\\n",
        "        + f'{norm_context} != {norm_context_org}'\n",
        "    assert len(norm_context) == len(offsets_norm_context), \\\n",
        "        'Normalized contexts have different numbers of tokens: ' \\\n",
        "        + f'{len(norm_context)} != {len(offsets_norm_context)}'\n",
        "\n",
        "    # 2. Make offsets of tokens (input_ids) within the normalized context.\n",
        "    offsets_token = []\n",
        "    unk_pointer = None\n",
        "    cid = 0\n",
        "    tid = 0\n",
        "    while tid < len(tokens):\n",
        "        cur_token = tokens[tid]\n",
        "        if cur_token == tokenizer.unk_token:\n",
        "            unk_pointer = tid\n",
        "            offsets_token.append([cid, cid])\n",
        "            cid += 1\n",
        "        elif norm_context[cid:cid + len(cur_token)] != cur_token:\n",
        "            # Wrong offsets of the previous UNK token\n",
        "            assert unk_pointer is not None, \\\n",
        "                'Normalized context and tokens are not matched'\n",
        "            prev_unk_expected = offsets_token[unk_pointer]\n",
        "            prev_unk_expected[1] += norm_context[prev_unk_expected[1] + 2:]\\\n",
        "                .index(tokens[unk_pointer + 1]) + 1\n",
        "            tid = unk_pointer\n",
        "            offsets_token = offsets_token[:tid] + [prev_unk_expected]\n",
        "            cid = prev_unk_expected[1] + 1\n",
        "        else:\n",
        "            start_pos = norm_context[cid:].index(cur_token)\n",
        "            if start_pos > 0 and tokens[tid - 1] == tokenizer.unk_token:\n",
        "                offsets_token[-1][1] += start_pos\n",
        "                cid += start_pos\n",
        "                start_pos = 0\n",
        "            assert start_pos == 0, f'{start_pos} != 0 (cur: {cur_token}'\n",
        "            offsets_token.append([cid, cid + len(cur_token) - 1])\n",
        "            cid += len(cur_token)\n",
        "            while cid < len(norm_context) and norm_context[cid] in whitespace:\n",
        "                offsets_token[-1][1] += 1\n",
        "                cid += 1\n",
        "        tid += 1\n",
        "    if tokens[-1] == tokenizer.unk_token:\n",
        "        offsets_token[-1][1] = len(norm_context) - 1\n",
        "    else:\n",
        "        assert cid == len(norm_context) == offsets_token[-1][1] + 1, \\\n",
        "            'Offsets do not include all characters'\n",
        "    assert len(offsets_token) == len(tokens), \\\n",
        "        'The numbers of tokens and offsets are different'\n",
        "\n",
        "    offsets_mapping = [(offsets_norm_context[start], offsets_norm_context[end])\n",
        "                       for start, end in offsets_token]\n",
        "    return [(-1, -1)] * cxt_start + offsets_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "erLcpPrsYJTN"
      },
      "outputs": [],
      "source": [
        "class QAModel(LightningModule):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()  # kwargs are saved in self.hparams\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "        self.question_answerer = AutoModelForQuestionAnswering.from_pretrained(\n",
        "            self.hparams.pretrained_model,\n",
        "            use_auth_token=self.hparams.huggingface_auth_token)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.hparams.pretrained_tokenizer\n",
        "            if self.hparams.pretrained_tokenizer else\n",
        "            self.hparams.pretrained_model,\n",
        "            use_auth_token=self.hparams.huggingface_auth_token)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        return self.question_answerer(**kwargs)\n",
        "\n",
        "    def step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        start_preds = outputs.start_logits.argmax(dim=-1).cpu().detach()\n",
        "        end_preds = outputs.end_logits.argmax(dim=-1).cpu().detach()\n",
        "        start_positions = batch['start_positions'].cpu().detach()\n",
        "        end_positions = batch['end_positions'].cpu().detach()\n",
        "\n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'start_preds': start_preds,\n",
        "            'end_preds': end_preds,\n",
        "            'start_positions': start_positions,\n",
        "            'end_positions': end_positions,\n",
        "        }\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        opt = self.optimizers()\n",
        "        opt.zero_grad()\n",
        "        outputs = self.step(batch, batch_idx)\n",
        "        self.manual_backward(outputs['loss'])\n",
        "        opt.step()\n",
        "\n",
        "        # single scheduler\n",
        "        sch = self.lr_schedulers()\n",
        "        sch.step()\n",
        "        return outputs\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.step(batch, batch_idx)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_f1_score(start_positions, end_positions, start_preds,\n",
        "                           end_preds):\n",
        "        start_overlap = np.maximum(start_positions, start_preds)\n",
        "        start_overlap = np.maximum(start_positions, start_preds)\n",
        "        end_overlap = np.minimum(end_positions, end_preds)\n",
        "        overlap = np.maximum(end_overlap - start_overlap + 1, 0)\n",
        "\n",
        "        pred_token_count = np.maximum(end_preds - start_preds + 1, 0)\n",
        "        ground_token_count = np.maximum(end_positions - start_positions + 1, 0)\n",
        "\n",
        "        precision = torch.nan_to_num(overlap / pred_token_count, nan=0.)\n",
        "        recall = torch.nan_to_num(overlap / ground_token_count, nan=0.)\n",
        "        f1 = torch.nan_to_num(\n",
        "            2 * precision * recall / (precision + recall), nan=0.)\n",
        "        return {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_exact_match(start_positions, end_positions, start_preds,\n",
        "                              end_preds):\n",
        "        equal_start = (start_preds == start_positions)\n",
        "        equal_end = (end_preds == end_positions)\n",
        "        return (equal_start * equal_end).type(torch.float)\n",
        "\n",
        "    def epoch_end(self, outputs, state='train'):\n",
        "        loss = torch.tensor(0, dtype=torch.float)\n",
        "        precision = torch.tensor(0, dtype=torch.float)\n",
        "        recall = torch.tensor(0, dtype=torch.float)\n",
        "        f1 = torch.tensor(0, dtype=torch.float)\n",
        "        em = torch.tensor(0, dtype=torch.float)\n",
        "\n",
        "        for i in outputs:\n",
        "            loss += i['loss'].cpu().detach()\n",
        "            f1_metrics = self.calculate_f1_score(i['start_positions'],\n",
        "                                                 i['end_positions'],\n",
        "                                                 i['start_preds'],\n",
        "                                                 i['end_preds'])\n",
        "            precision += f1_metrics['precision'].mean()\n",
        "            recall += f1_metrics['recall'].mean()\n",
        "            f1 += f1_metrics['f1'].mean()\n",
        "            em += self.calculate_exact_match(i['start_positions'],\n",
        "                                             i['end_positions'],\n",
        "                                             i['start_preds'],\n",
        "                                             i['end_preds']).mean()\n",
        "        loss = loss / len(outputs)\n",
        "        precision = precision / len(outputs)\n",
        "        recall = recall / len(outputs)\n",
        "        f1 = f1 / len(outputs)\n",
        "        em = em / len(outputs)\n",
        "        metrics = {\n",
        "            state + '_loss': float(loss),\n",
        "            state + '_precision': precision,\n",
        "            state + '_recall': recall,\n",
        "            state + '_f1': f1,\n",
        "            state + '_em': em,\n",
        "        }\n",
        "\n",
        "        self.log_dict(metrics, on_epoch=True)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        self.epoch_end(outputs, state='train')\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self.epoch_end(outputs, state='val')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.hparams.optimizer == 'AdamW':\n",
        "            optimizer = AdamW(\n",
        "                self.parameters(),\n",
        "                lr=self.hparams.lr,\n",
        "                weight_decay=self.hparams.weight_decay)\n",
        "        else:\n",
        "            raise NotImplementedError('Only AdamW is Supported!')\n",
        "\n",
        "        if self.hparams.lr_scheduler == 'cos':\n",
        "            scheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                optimizer, T_0=1, T_mult=2)\n",
        "        elif self.hparams.lr_scheduler == 'exp':\n",
        "            scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
        "        elif self.hparams.lr_scheduler == 'warmup_lin':\n",
        "            steps_per_epoch = len(self.train_dataloader())\n",
        "            if os.environ.get('COLAB_TPU_ADDR'):\n",
        "                steps_per_epoch = steps_per_epoch // self.hparams.tpu_cores\n",
        "            total_steps = steps_per_epoch * self.hparams.epochs\n",
        "            warmup_steps = int(total_steps * self.hparams.warmup_ratio)\n",
        "            scheduler = WarmupLinearLR(\n",
        "                optimizer, warmup_steps=warmup_steps, max_steps=total_steps)\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                'Only cos, exp, and warmup_lin lr scheduler is Supported!')\n",
        "\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def preprocess_function(self, examples):\n",
        "        tokenized_examples = self.tokenizer(\n",
        "            examples['question'],\n",
        "            examples['context'],\n",
        "        )\n",
        "\n",
        "        inputs = {\n",
        "            'input_ids': [],\n",
        "            'attention_mask': [],\n",
        "            'token_type_ids': [],\n",
        "            'start_positions': [],\n",
        "            'end_positions': [],\n",
        "        }\n",
        "        for tokens, att_mask, type_ids, context, answer, start_char \\\n",
        "                in zip(tokenized_examples['input_ids'],\n",
        "                       tokenized_examples['attention_mask'],\n",
        "                       tokenized_examples['token_type_ids'],\n",
        "                       examples['context'],\n",
        "                       examples['answer'],\n",
        "                       examples['answer_start']):\n",
        "            answer = answer[0]\n",
        "            start_char = start_char[0]\n",
        "            offsets = get_offsets(tokens, context, self.tokenizer,\n",
        "                                  self.hparams.norm_form)\n",
        "\n",
        "            ctx_start = tokens.index(self.tokenizer.sep_token_id) + 1\n",
        "            answer_start_index = ctx_start\n",
        "            answer_end_index = len(offsets) - 1\n",
        "            while offsets[answer_start_index][0] < start_char:\n",
        "                answer_start_index += 1\n",
        "            while offsets[answer_end_index][1] > start_char + len(answer):\n",
        "                answer_end_index -= 1\n",
        "\n",
        "            span_inputs = {\n",
        "                'input_ids': tokens,\n",
        "                'attention_mask': att_mask,\n",
        "                'token_type_ids': type_ids,\n",
        "            }\n",
        "            for span, answer_idx in make_spans(\n",
        "                    span_inputs,\n",
        "                    question_len=ctx_start,\n",
        "                    max_seq_len=self.hparams.max_length,\n",
        "                    stride=self.hparams.doc_stride,\n",
        "                    answer_start_position=answer_start_index,\n",
        "                    answer_end_position=answer_end_index):\n",
        "                inputs['input_ids'].append(span['input_ids'])\n",
        "                inputs['attention_mask'].append(span['attention_mask'])\n",
        "                inputs['token_type_ids'].append(span['token_type_ids'])\n",
        "                inputs['start_positions'].append(answer_idx[0])\n",
        "                inputs['end_positions'].append(answer_idx[1])\n",
        "        return inputs\n",
        "\n",
        "    def prepare_data(self):\n",
        "        datasetdict = datasets.load_dataset(\n",
        "            self.hparams.dataset,\n",
        "            use_auth_token=self.hparams.huggingface_auth_token)\n",
        "        datasetdict = datasetdict.flatten()\\\n",
        "            .rename_column('answers.text', 'answer')\\\n",
        "            .rename_column('answers.answer_start', 'answer_start')\\\n",
        "            .rename_column('answers.answer_type', 'answer_type')\n",
        "\n",
        "        self.tokenized_dataset = datasetdict.map(\n",
        "            self.preprocess_function,\n",
        "            batched=True,\n",
        "            remove_columns=datasetdict['train'].column_names)\n",
        "\n",
        "    def dataloader(self, dataset, shuffle=False):\n",
        "        dataset.set_format(type='torch')\n",
        "\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=self.hparams.cpu_workers,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.dataloader(self.tokenized_dataset['train'], shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.dataloader(\n",
        "            self.tokenized_dataset['validation'], shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN41xPPunWQJ"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZzpIpf64rZsA"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import callbacks\n",
        "from pytorch_lightning import loggers\n",
        "\n",
        "checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filename='val_loss{val_loss:.4f}-val_f1{val_f1:.4f}-epoch{epoch}',\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_top_k=5,\n",
        "    auto_insert_metric_name=False,\n",
        ")\n",
        "lr_callback = callbacks.LearningRateMonitor(logging_interval='step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aPXGrWBnWQL",
        "outputId": "dc9a0eee-2b45-46c9-e980-2c2d8ab3859e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyTorch Ver 1.10.0\n",
            "Fix Seed: 42\n"
          ]
        },
        {
          "ename": "MisconfigurationException",
          "evalue": "No TPU devices were found.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2899/1115143511.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# precision=16 if args['fp16'] and torch.cuda.is_available() else 32,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# For TPU Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtpu_cores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tpu_cores'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tpu_cores'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m )\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mgpu_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_cores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_select_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# init connectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_parse_devices\u001b[0;34m(gpus, auto_select_gpus, tpu_cores)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;31m# TODO (@seannaren, @kaushikb11): Include IPU parsing logic here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0mgpu_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m         \u001b[0mtpu_cores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tpu_cores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgpu_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_cores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36mparse_tpu_cores\u001b[0;34m(tpu_cores)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtpu_cores\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_TPU_AVAILABLE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No TPU devices were found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtpu_cores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMisconfigurationException\u001b[0m: No TPU devices were found."
          ]
        }
      ],
      "source": [
        "print('Using PyTorch Ver', torch.__version__)\n",
        "print('Fix Seed:', args['random_seed'])\n",
        "seed_everything(args['random_seed'])\n",
        "\n",
        "model = QAModel(**args)\n",
        "\n",
        "trainer = Trainer(\n",
        "    callbacks=[lr_callback,\n",
        "               checkpoint_callback],\n",
        "    log_every_n_steps=16,  # Logging frequency of **learning rate**\n",
        "    max_epochs=args['epochs'],\n",
        "    fast_dev_run=args['test_mode'],\n",
        "    num_sanity_val_steps=None if args['test_mode'] else 0,\n",
        "    # For GPU Setup\n",
        "    # deterministic=torch.cuda.is_available(),\n",
        "    # gpus=[0] if torch.cuda.is_available() else None,  # Use one GPU (idx 0)\n",
        "    # precision=16 if args['fp16'] and torch.cuda.is_available() else 32,\n",
        "    # For TPU Setup\n",
        "    tpu_cores=args['tpu_cores'] if args['tpu_cores'] else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es7nKQ3w26vq"
      },
      "outputs": [],
      "source": [
        "print(':: Start Training ::')\n",
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XbLPtuPY-hJ"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqAgK6JVnWQM"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTNeTGApNlWf"
      },
      "outputs": [],
      "source": [
        "# Load fine-tuned model from huggingface (fine-tuned by Skelter Labs)\n",
        "# If you are trying to use your model, skip this cell.\n",
        "!apt-get install git-lfs\n",
        "\n",
        "args['pretrained_model'] = 'SkelterLabsInc/bert-base-japanese-jaquad'\n",
        "model = QAModel(**args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTpDsD8-bapF"
      },
      "outputs": [],
      "source": [
        "def get_answers(model: AutoModelForQuestionAnswering,\n",
        "                context: str,\n",
        "                question: str,\n",
        "                n_best_size: int = 5,\n",
        "                max_seq_len: int = 384,\n",
        "                doc_stride: int = 128) -> List[Dict[str, Any]]:\n",
        "    valid_answers = []\n",
        "    inputs = model.tokenizer(question, context)\n",
        "    offsets = get_offsets(inputs['input_ids'], context, model.tokenizer,\n",
        "                          model.hparams.norm_form)\n",
        "    question_len = inputs['input_ids'].index(model.tokenizer.sep_token_id) + 1\n",
        "    i = 0\n",
        "    for span, _ in make_spans(\n",
        "            inputs,\n",
        "            question_len=question_len,\n",
        "            max_seq_len=max_seq_len,\n",
        "            stride=doc_stride):\n",
        "        for key, val in span.items():\n",
        "            span[key] = torch.Tensor([val]).type(torch.long)\n",
        "        output = model(**span)\n",
        "        start_logits = output.start_logits[0].cpu().detach().numpy()\n",
        "        end_logits = output.end_logits[0].cpu().detach().numpy()\n",
        "        start_indexes = np.argsort(start_logits)[-1:-n_best_size -\n",
        "                                                 1:-1].tolist()\n",
        "        end_indexes = np.argsort(end_logits)[-1:-n_best_size - 1:-1].tolist()\n",
        "        cur_offsets = offsets[i:]\n",
        "        i += doc_stride\n",
        "        for start_index in start_indexes:\n",
        "            for end_index in end_indexes:\n",
        "                if 0 < start_index <= end_index < len(cur_offsets):\n",
        "                    # We need to refine that test to check the answer is inside\n",
        "                    # the context\n",
        "                    valid_answers.append({\n",
        "                        'score':\n",
        "                            start_logits[start_index] + end_logits[end_index],\n",
        "                        'position': (start_index, end_index),\n",
        "                        'text':\n",
        "                            context[cur_offsets[start_index][0]:\n",
        "                                    cur_offsets[end_index - 1][1] + 1],\n",
        "                    })\n",
        "    if not valid_answers:\n",
        "        return [{\n",
        "            'score': -float('inf'),\n",
        "            'position': (-1, -1),\n",
        "            'text': '',\n",
        "        }]\n",
        "    valid_answers.sort(key=lambda x: x['score'], reverse=True)\n",
        "    return valid_answers[:n_best_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k1DIdDYg5_N"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def compute_f1_score(ground_truth_values: List[str],\n",
        "                     prediction_values: List[str]) -> float:\n",
        "    '''Compute f1 score comparing two list of values.'''\n",
        "    common = (\n",
        "        collections.Counter(prediction_values) &\n",
        "        collections.Counter(ground_truth_values))\n",
        "    num_same = sum(common.values())\n",
        "\n",
        "    # No answer case.\n",
        "    if not ground_truth_values or not prediction_values:\n",
        "        return int(ground_truth_values == prediction_values)\n",
        "\n",
        "    if num_same == 0:\n",
        "        return 0.\n",
        "\n",
        "    precision = 1.0 * num_same / len(prediction_values)\n",
        "    recall = 1.0 * num_same / len(ground_truth_values)\n",
        "    f1_score = (2 * precision * recall) / (precision + recall)\n",
        "    return f1_score\n",
        "\n",
        "\n",
        "def char_f1_score(prediction: str, ground_truth: str) -> float:\n",
        "    '''Character F1 score.'''\n",
        "    prediction_tokens = prediction.split()\n",
        "    ground_truth_tokens = ground_truth.split()\n",
        "\n",
        "    # F1 by character\n",
        "    prediction_char = []\n",
        "    for tok in prediction_tokens:\n",
        "        prediction_char.extend(list(tok))\n",
        "\n",
        "    ground_truth_char = []\n",
        "    for tok in ground_truth_tokens:\n",
        "        ground_truth_char.extend(list(tok))\n",
        "    return compute_f1_score(ground_truth_char, prediction_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uKGSIdi0skje"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset ja_qu_ad (/home/s16991/.cache/huggingface/datasets/SkelterLabsInc___ja_qu_ad/default/0.1.0/5847b2e2ab5e02de284395bb15f87f13eae8f6f6ff1f01e4ee9c5c0dcf8ef8eb)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd5057691daa4f4fbacfc394b0fd84c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval 3939 data\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'get_answers' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_29581/3816158327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             infer_data['answer'])):\n\u001b[1;32m     15\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mpred_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_answers' is not defined"
          ]
        }
      ],
      "source": [
        "jaquad_dataset = datasets.load_dataset(\n",
        "    args['dataset'], use_auth_token=args['huggingface_auth_token']).flatten()\n",
        "jaquad_dataset = jaquad_dataset.rename_column('answers.text', 'answer')\n",
        "\n",
        "infer_data = jaquad_dataset['validation'][:]\n",
        "\n",
        "cnt = len(infer_data['question'])\n",
        "print(f'Eval {cnt} data')\n",
        "\n",
        "f1_scores = []\n",
        "em = []\n",
        "for i, (context, question, answer) in enumerate(\n",
        "        zip(infer_data['context'], infer_data['question'],\n",
        "            infer_data['answer'])):\n",
        "    answer = answer[0]\n",
        "    predictions = get_answers(model, context=context, question=question)\n",
        "    pred_text = predictions[0]['text']\n",
        "    f1 = char_f1_score(pred_text, answer)\n",
        "    f1_scores.append(f1)\n",
        "    em.append(f1 == 1.)\n",
        "    if i % 200 == 0:\n",
        "        print(f'  {i+1}/{cnt} | EM: {sum(em) / (i+1):.4f}, '\\\n",
        "              + f'F1: {sum(f1_scores) / (i+1):.4f}')\n",
        "        print(f'        (Sample) pred: \"{pred_text}\", answer: \"{answer}\"')\n",
        "print(f'F1 score: {sum(f1_scores) / cnt}')\n",
        "print(f'Exact Match: {sum(em) / cnt}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khT4Jiln92Tb"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "Following cells generate graphs used in the paper \"JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pzzMnITpcAfO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset ja_qu_ad (/home/s16991/.cache/huggingface/datasets/SkelterLabsInc___ja_qu_ad/default/0.1.0/5847b2e2ab5e02de284395bb15f87f13eae8f6f6ff1f01e4ee9c5c0dcf8ef8eb)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0040374f4abd4def9c80241ccb22119b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import datasets\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "args['pretrained_model'] = 'SkelterLabsInc/bert-base-japanese-jaquad'\n",
        "model = QAModel(**args)\n",
        "\n",
        "jaquad = datasets.load_dataset(\n",
        "    args['dataset'],\n",
        "    use_auth_token=args['huggingface_auth_token']).flatten()\n",
        "\n",
        "tokenizer = model.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = jaquad[\"train\"]\n",
        "train_data.set_format(type='pandas')\n",
        "train = pd.DataFrame(train_data)\n",
        "train \n",
        "# jaquad[\"train\"].set_format(type='pandas')\n",
        "# train_data = jaquad[\"train\"]\n",
        "# train_data = train_data.data\n",
        "# type(train_data)\n",
        "# train = pd.DataFrame(train_data)\n",
        "#jaquad_pd.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdkqrdc694OI"
      },
      "source": [
        "### Context/Answer lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubY_ftnV916m",
        "outputId": "57efdc26-c7ee-43cc-99b5-ea1e5c6c21e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "context_lengths = []\n",
        "question_lengths = []\n",
        "answer_lengths = []\n",
        "\n",
        "for dset in jaquad.values():\n",
        "    for batch in dset:\n",
        "        context_tokens = tokenizer(batch['context'])['input_ids']\n",
        "        context_lengths.append(len(context_tokens) - 2)\n",
        "        question_tokens = tokenizer(batch['question'])['input_ids']\n",
        "        question_lengths.append(len(question_tokens) - 2)\n",
        "        answer_tokens = tokenizer(batch['answers.text'][0])['input_ids']\n",
        "        answer_lengths.append(len(answer_tokens) - 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "15dPWXpLgM00",
        "outputId": "86dbbb3c-3d13-4740-b89f-cbf137cfc9c5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Context lengths\n",
        "\n",
        "bin_size = 100\n",
        "bins = range(1, max(context_lengths) + bin_size, bin_size)\n",
        "plt.hist(context_lengths,\n",
        "         bins,\n",
        "         density=False,\n",
        "         facecolor='g',\n",
        "         alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MpvnzjoIx1lt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 100]         2422\n",
              "(100, 200]       9871\n",
              "(200, 300]      10921\n",
              "(300, 400]       6899\n",
              "(400, 500]       3515\n",
              "(500, 600]       1050\n",
              "(600, 700]        467\n",
              "(700, 800]        266\n",
              "(800, 900]        262\n",
              "(900, 1000]         5\n",
              "(1000, 1100]        6\n",
              "(1100, 1200]        3\n",
              "Name: bin, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bins = range(0, max(context_lengths) + bin_size, bin_size)\n",
        "df = pd.DataFrame({'context_lengths': context_lengths})\n",
        "df['bin'] = pd.cut(df.context_lengths, bins)\n",
        "df.bin.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "YP-rRk6FmrGo",
        "outputId": "f4ed8639-e07c-4adc-d5ce-96c959947611"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Question lengths\n",
        "\n",
        "bin_size = 10\n",
        "bins = range(1, max(question_lengths) + bin_size, bin_size)\n",
        "plt.hist(question_lengths,\n",
        "        bins,\n",
        "        density=False,\n",
        "        facecolor='r',\n",
        "        alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KTgMc7xXbC04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 10]        1623\n",
              "(10, 20]      18041\n",
              "(20, 30]      11767\n",
              "(30, 40]       3224\n",
              "(40, 50]        759\n",
              "(50, 60]        176\n",
              "(60, 70]         67\n",
              "(70, 80]         18\n",
              "(80, 90]          5\n",
              "(90, 100]         2\n",
              "(100, 110]        1\n",
              "(110, 120]        4\n",
              "Name: bin, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bins = range(0, max(question_lengths) + bin_size, bin_size)\n",
        "df = pd.DataFrame({'question_lengths': question_lengths})\n",
        "df['bin'] = pd.cut(df.question_lengths, bins)\n",
        "df.bin.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "EQl_S6-VEtDX",
        "outputId": "598e87d1-c17c-4358-d68b-c984da1d0d2c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Answer lengths\n",
        "\n",
        "bin_size = 2\n",
        "bins = range(1, max(answer_lengths) + bin_size, bin_size)\n",
        "plt.hist([a for a in answer_lengths if a <= 20],\n",
        "         bins[:10],\n",
        "         density=False,\n",
        "         facecolor='b',\n",
        "         alpha=0.5)\n",
        "plt.xticks(bins[:10])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "psu7iHwf0N38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 2]      15908\n",
              "(2, 4]      12389\n",
              "(4, 6]       4908\n",
              "(6, 8]       1569\n",
              "(8, 10]       535\n",
              "(10, 12]      170\n",
              "(12, 14]       79\n",
              "(14, 16]       49\n",
              "(16, 18]       35\n",
              "(18, 20]       12\n",
              "(20, 22]        8\n",
              "(22, 24]        5\n",
              "(24, 26]        5\n",
              "(28, 30]        4\n",
              "(30, 32]        3\n",
              "(38, 40]        2\n",
              "(34, 36]        2\n",
              "(32, 34]        1\n",
              "(40, 42]        1\n",
              "(68, 70]        1\n",
              "(26, 28]        1\n",
              "(42, 44]        0\n",
              "(44, 46]        0\n",
              "(46, 48]        0\n",
              "(48, 50]        0\n",
              "(50, 52]        0\n",
              "(36, 38]        0\n",
              "(54, 56]        0\n",
              "(56, 58]        0\n",
              "(58, 60]        0\n",
              "(60, 62]        0\n",
              "(62, 64]        0\n",
              "(64, 66]        0\n",
              "(66, 68]        0\n",
              "(52, 54]        0\n",
              "Name: bin, dtype: int64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bins = range(0, max(answer_lengths) + bin_size, bin_size)\n",
        "\n",
        "df = pd.DataFrame({'answer_lengths': answer_lengths})\n",
        "df['bin'] = pd.cut(df.answer_lengths, bins)\n",
        "df.bin.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcOaM1HQHo4e"
      },
      "source": [
        "### Performance by question/answer types and answer_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Lp9TACgPMYZG"
      },
      "outputs": [],
      "source": [
        "dev_answer_lengths = []\n",
        "for datum in jaquad['validation']:\n",
        "    answer = tokenizer(datum['answers.text'])['input_ids']\n",
        "    dev_answer_lengths.append(len(answer) - 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LczWxMSnH02f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'f1_scores' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_29581/3411856291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'answer_type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaquad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answers.answer_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'answer_len'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdev_answer_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m'f1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf1_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m'em'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m })\n",
            "\u001b[0;31mNameError\u001b[0m: name 'f1_scores' is not defined"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame({\n",
        "    'question_type': jaquad['validation']['question_type'],\n",
        "    'answer_type': [typ[0] for typ in jaquad['validation']['answers.answer_type']],\n",
        "    'answer_len': dev_answer_lengths,\n",
        "    'f1': f1_scores,\n",
        "    'em': em,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsJ2gKRoH6Bq"
      },
      "outputs": [],
      "source": [
        "df[['question_type', 'f1', 'em']].groupby(by='question_type').mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_nM9S4TLt3K"
      },
      "outputs": [],
      "source": [
        "df[['answer_type', 'f1', 'em']].groupby(by='answer_type').mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxlfAcEGH8eq"
      },
      "outputs": [],
      "source": [
        "bins = 2\n",
        "\n",
        "df['answer_len_bin'] = (df['answer_len']-1) // bins * bins\n",
        "df['answer_len_bin'][df.answer_len_bin >= 8] = 8\n",
        "df['answer_len_bin'] = df['answer_len_bin'].apply(lambda x: f'{x+1:02d}-{x+bins:02d}')\n",
        "df[['answer_len_bin', 'f1', 'em']].groupby(by='answer_len_bin').mean()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "JaQuAD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
