{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "from typing import Any, Dict, Iterator, List, Tuple, Union\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import lr_scheduler\n",
    "from transformers import BertJapaneseTokenizer,ElectraTokenizer,T5Tokenizer\n",
    "import transformers \n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning import loggers\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AdamW\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'random_seed': 42,  # Random Seed\n",
    "    # Transformers PLM name.\n",
    "    'pretrained_model': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "    # Optional, Transformers Tokenizer name. Overrides `pretrained_model`\n",
    "    'pretrained_tokenizer': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "    'norm_form': 'NFKC',\n",
    "    'batch_size': 8,  # <=32 for TPUv2-8\n",
    "    'lr': 2e-5,  # Learning Rate\n",
    "    'max_length': 384,  # Max Length input size\n",
    "    'doc_stride': 128,  # The interval of the context when splitting is needed\n",
    "    'epochs': 3,  # Max Epochs\n",
    "    'dataset': 'SkelterLabsInc/JaQuAD',\n",
    "    'huggingface_auth_token': None,\n",
    "    'test_mode': False,  # Test Mode enables `fast_dev_run`\n",
    "    'optimizer': 'AdamW',\n",
    "    'weight_decay': 0.01,  # Weight decaying parameter for AdamW\n",
    "    'lr_scheduler': 'warmup_lin',\n",
    "    'warmup_ratio': 0.1,\n",
    "    'fp16': False,  # Enable train on FP16 (if GPU)\n",
    "    'tpu_cores': 8,  # Enable TPU with 1 core or 8 cores\n",
    "    'cpu_workers': os.cpu_count(), #questionとcontextを合わせた最大語数。今回のデータセットは1300は超えない\n",
    "    'note':\"リクルートベースライン\",\n",
    "}\n",
    "\n",
    "args\n",
    "\n",
    "#seed値を固定\n",
    "def set_seed(seed =42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "set_seed(seed=args[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetdict = datasets.load_dataset(\n",
    "    args['dataset'], use_auth_token=args['huggingface_auth_token'])\n",
    "datasetdict = datasetdict.flatten()\\\n",
    "            .rename_column('answers.text', 'answer')\\\n",
    "            .rename_column('answers.answer_start', 'answer_start')\\\n",
    "            .rename_column('answers.answer_type', 'answer_type')\n",
    "train =pd.DataFrame(datasetdict['train'][:].values(), index=datasetdict['train'][:].keys()).T\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context= train[\"context\"][100:105]\n",
    "#sample_context= [train[\"context\"][0]]\n",
    "sample_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "BertJapanese_tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "T5Tokenizer_tokenizer=T5Tokenizer.from_pretrained('rinna/japanese-roberta-base',model_max_length=384)\n",
    "electra_tokenizer = AutoTokenizer.from_pretrained(\"Cinnamon/electra-small-japanese-generator\")\n",
    "electra_tokenizer = AutoTokenizer.from_pretrained(\"Cinnamon/electra-small-japanese-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokneize_QA(question,text,Tokenizer= AutoTokenizer,pretrained_tokenizer = \"cl-tohoku/bert-base-japanese-whole-word-masking\"):\n",
    "    Tokenizer = Tokenizer.from_pretrained(pretrained_tokenizer)\n",
    "    input_ids = Tokenizer.encode(question, text)\n",
    "    tokens = Tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(pretrained_tokenizer = \"cl-tohoku/bert-base-japanese-whole-word-masking\",Tokenizer= AutoTokenizer,contexts= sample_context):\n",
    "    Tokenizer = Tokenizer.from_pretrained(pretrained_tokenizer)\n",
    "    for i in range(len(contexts)):\n",
    "        print(\"オリジナル\")\n",
    "        print(contexts[i])\n",
    "        print(pretrained_tokenizer)\n",
    "        encoding = Tokenizer.encode(contexts[i])\n",
    "        tokens = Tokenizer.convert_ids_to_tokens(encoding)\n",
    "        print(tokens)\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(pretrained_tokenizer = \"cl-tohoku/bert-base-japanese-whole-word-masking\",Tokenizer= AutoTokenizer)\n",
    "tokenizer(pretrained_tokenizer = \"rinna/japanese-roberta-base\",Tokenizer= AutoTokenizer)\n",
    "# tokenizer(pretrained_tokenizer = \"rinna/japanese-roberta-base\",Tokenizer= BertJapaneseTokenizer)\n",
    "tokenizer(pretrained_tokenizer = \"rinna/japanese-roberta-base\",Tokenizer= T5Tokenizer)\n",
    "# tokenizer(pretrained_tokenizer = \"cl-tohoku/bert-base-japanese-whole-word-masking\",Tokenizer= AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#公式サイトで指示されている通りに使い分ける。\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "            args[\"pretrained_tokenizer\"]\n",
    "            if args[\"pretrained_tokenizer\"] else\n",
    "            args[\"pretrained_model\"])\n",
    "\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "#detasetdictをbertに入る形に加工\n",
    "def preprocess_function(examples):\n",
    "    #長文を入れた時に、二つに分ける処理は未実装\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['question' if pad_on_right else \"context\"],\n",
    "        examples['context' if pad_on_right else \"question\"],\n",
    "        return_overflowing_tokens=True, \n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    inputs = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'start_positions': [],\n",
    "        'end_positions': [],\n",
    "    }\n",
    "    \n",
    "    for tokens, att_mask, type_ids, context, answer,question,start_char \\\n",
    "            in zip(tokenized_examples['input_ids'],\n",
    "                    tokenized_examples['attention_mask'],\n",
    "                    tokenized_examples['token_type_ids'],\n",
    "                    examples['context'],\n",
    "                    examples['answer'],\n",
    "                    examples['question'],\n",
    "                    examples['answer_start']):\n",
    "                \n",
    "        #答えの場所をトークンで測るとどこになるかを計算\n",
    "        answer = answer[0]\n",
    "        start_char = start_char[0]\n",
    "        offsets = get_offsets(tokens, context, tokenizer,\n",
    "                                args[\"norm_form\"])\n",
    "        ctx_start = tokens.index(tokenizer.sep_token_id) + 1\n",
    "        answer_start_index = 0\n",
    "        answer_end_index = len(offsets) - 2\n",
    "        while offsets[answer_start_index][0] < start_char:\n",
    "            answer_start_index += 1\n",
    "        while offsets[answer_end_index][1] > start_char + len(answer):\n",
    "            answer_end_index -= 1\n",
    "        answer_start_index += ctx_start\n",
    "        answer_end_index += ctx_start\n",
    "\n",
    "        span_inputs = {\n",
    "            'input_ids': tokens,\n",
    "            'attention_mask': att_mask,\n",
    "            'token_type_ids': type_ids,\n",
    "        }\n",
    "        for span, answer_idx in make_spans(\n",
    "                span_inputs,\n",
    "                question_len=ctx_start,\n",
    "                max_seq_len=args[\"max_length\"],\n",
    "                stride=args[\"doc_stride\"],\n",
    "                answer_start_position=answer_start_index,\n",
    "                answer_end_position=answer_end_index):\n",
    "            inputs['input_ids'].append(span['input_ids'])\n",
    "            inputs['attention_mask'].append(span['attention_mask'])\n",
    "            inputs['start_positions'].append(answer_idx[0])\n",
    "            inputs['end_positions'].append(answer_idx[1])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def make_spans(\n",
    "    inputs: Dict[str, Union[int, List[int]]],\n",
    "    question_len: int,\n",
    "    max_seq_len: int,\n",
    "    stride: int,\n",
    "    answer_start_position: int = -1,\n",
    "    answer_end_position: int = -1\n",
    ") -> Iterator[Tuple[Dict[str, List[int]], Tuple[int, int]]]:\n",
    "    input_len = len(inputs['input_ids'])\n",
    "    context_len = input_len - question_len\n",
    "\n",
    "    def make_value(input_list, i, padding=0):\n",
    "        context_end = min(max_seq_len - question_len, context_len - i)\n",
    "        pad_len = max_seq_len - question_len - context_end\n",
    "        val = input_list[:question_len]\n",
    "        val += input_list[question_len + i:question_len + i + context_end]\n",
    "        val[-1] = input_list[-1]\n",
    "        val += [padding] * pad_len\n",
    "        return val\n",
    "\n",
    "    for i in range(0, input_len - max_seq_len + stride, stride):\n",
    "        span = {key: make_value(val, i) for key, val in inputs.items()}\n",
    "        answer_start = answer_start_position - i\n",
    "        answer_end = answer_end_position - i\n",
    "        if answer_start < question_len or answer_end >= max_seq_len - 1:\n",
    "            answer_start = answer_end = 0\n",
    "        yield span, (answer_start, answer_end)\n",
    "        \n",
    "\n",
    "def get_offsets(input_ids: List[int],\n",
    "                context: str,\n",
    "                tokenizer: AutoTokenizer,\n",
    "                norm_form='NFKC') -> List[Tuple[int, int]]:\n",
    "    \n",
    "    cxt_start = input_ids.index(tokenizer.sep_token_id) + 1\n",
    "    cxt_end = cxt_start + input_ids[cxt_start:].index(tokenizer.sep_token_id)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[cxt_start:cxt_end])\n",
    "    tokens = [tok[2:] if tok.startswith('##') else tok for tok in tokens]\n",
    "    whitespace = string.whitespace + '\\u3000'\n",
    "\n",
    "    # 1 . Make offsets of normalized context within the original context.\n",
    "    offsets_norm_context = []\n",
    "    norm_context = ''\n",
    "    for idx, char in enumerate(context):\n",
    "        norm_char = unicodedata.normalize(norm_form, char)\n",
    "        norm_context += norm_char\n",
    "        offsets_norm_context.extend([idx] * len(norm_char))\n",
    "    norm_context_org = unicodedata.normalize(norm_form, context)\n",
    "    assert norm_context == norm_context_org, \\\n",
    "        'Normalized contexts are not the same: ' \\\n",
    "        + f'{norm_context} != {norm_context_org}'\n",
    "    assert len(norm_context) == len(offsets_norm_context), \\\n",
    "        'Normalized contexts have different numbers of tokens: ' \\\n",
    "        + f'{len(norm_context)} != {len(offsets_norm_context)}'\n",
    "    \n",
    "    offsets_token = []\n",
    "    unk_pointer = None\n",
    "    cid = 0\n",
    "    tid = 0\n",
    "    while tid < len(tokens):\n",
    "        cur_token = tokens[tid]\n",
    "        if cur_token == tokenizer.unk_token:\n",
    "            unk_pointer = tid\n",
    "            offsets_token.append([cid, cid])\n",
    "            cid += 1\n",
    "        elif norm_context[cid:cid + len(cur_token)] != cur_token:\n",
    "            assert unk_pointer is not None, \\\n",
    "                'Normalized context and tokens are not matched'\n",
    "            prev_unk_expected = offsets_token[unk_pointer]\n",
    "            prev_unk_expected[1] += norm_context[prev_unk_expected[1] + 2:]\\\n",
    "                .index(tokens[unk_pointer + 1]) + 1\n",
    "            tid = unk_pointer\n",
    "            offsets_token = offsets_token[:tid] + [prev_unk_expected]\n",
    "            cid = prev_unk_expected[1] + 1\n",
    "        else:\n",
    "            start_pos = norm_context[cid:].index(cur_token)\n",
    "            if start_pos > 0 and tokens[tid - 1] == tokenizer.unk_token:\n",
    "                offsets_token[-1][1] += start_pos\n",
    "                cid += start_pos\n",
    "                start_pos = 0\n",
    "            assert start_pos == 0, f'{start_pos} != 0 (cur: {cur_token}'\n",
    "            offsets_token.append([cid, cid + len(cur_token) - 1])\n",
    "            cid += len(cur_token)\n",
    "            while cid < len(norm_context) and norm_context[cid] in whitespace:\n",
    "                offsets_token[-1][1] += 1\n",
    "                cid += 1\n",
    "        tid += 1\n",
    "    if tokens[-1] == tokenizer.unk_token:\n",
    "        offsets_token[-1][1] = len(norm_context) - 1\n",
    "    else:\n",
    "        ##長文を折り返すとここでエラーが出る。\n",
    "        assert cid == len(norm_context) == offsets_token[-1][1] + 1, \\\n",
    "            'Offsets do not include all characters'\n",
    "    assert len(offsets_token) == len(tokens), \\\n",
    "        'The numbers of tokens and offsets are different'\n",
    "\n",
    "    offsets_mapping = [(offsets_norm_context[start], offsets_norm_context[end])\n",
    "                       for start, end in offsets_token]\n",
    "    return [(0, 0)] + offsets_mapping+[(0, 0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テスト用\n",
    "features = preprocess_function(datasetdict['train'][100:105])\n",
    "#tokenized_datasets = datasetdict.map(preprocess_function, batched=True, remove_columns=datasetdict[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(pretrained_tokenizer = \"cl-tohoku/bert-base-japanese-whole-word-masking\",Tokenizer= AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens= tokneize_QA(train[\"question\"][102],train[\"context\"][102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[21:28]\n",
    "#  'start_positions': [24, 0, 48, 0, 21, 0, 52, 0, 36, 0],\n",
    "#  'end_positions': [50, 0, 52, 0, 28, 0, 53, 0, 41, 0]}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
