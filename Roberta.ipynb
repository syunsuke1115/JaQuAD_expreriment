{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robertaのノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import unicodedata\n",
    "from typing import Any, Dict, Iterator, List, Tuple, Union\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "import transformers \n",
    "from transformers import AutoTokenizer,AlbertTokenizer,BertJapaneseTokenizer,T5Tokenizer\n",
    "from transformers import AutoModelForQuestionAnswering,TrainingArguments,Trainer,ElectraForMaskedLM,RobertaForMaskedLM,AutoModel\n",
    "from transformers import default_data_collator\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7820fe2e024fa984bdacd9683b7358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rinna/japanese-roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at rinna/japanese-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "##ここを変更する\n",
    "args = {\n",
    "    'random_seed': 42,\n",
    "    'pretrained_model': 'rinna/japanese-roberta-base',\n",
    "    'pretrained_tokenizer': 'rinna/japanese-roberta-base',\n",
    "    'batch_size': 8, \n",
    "    \"eval_batch_size\":8,\n",
    "    'lr': 2e-5, \n",
    "    'max_length': 384,  \n",
    "    'doc_stride': 128,  \n",
    "    'epochs': 1,  \n",
    "    'dataset': 'SkelterLabsInc/JaQuAD',\n",
    "    'optimizer': 'AdamW',\n",
    "    'norm_form': 'NFKC',\n",
    "    'weight_decay': 0.01,  \n",
    "    'lr_scheduler': 'warmup_lin',\n",
    "    'warmup_ratio': 0.1,\n",
    "    \"eval_accumulation_steps\":10,\n",
    "    'cpu_workers': os.cpu_count(),\n",
    "    'note':\"same_prepare\",\n",
    "}\n",
    "args\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "            args[\"pretrained_tokenizer\"]\n",
    "            if args[\"pretrained_tokenizer\"] else\n",
    "            args[\"pretrained_model\"])\n",
    "# tokenizer.do_lower_case = True\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(args['pretrained_model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcashunsukechiba\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=3)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=3)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): DynamicQuantizedLinear(in_features=768, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seed値を固定\n",
    "def set_seed(seed =42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "set_seed(seed=args[\"random_seed\"])\n",
    "\n",
    "#wandb\n",
    "import wandb\n",
    "wandb.login()\n",
    "os.environ[\"WANDB_PROJECT\"] = \"JaQuad\"\n",
    "\n",
    "#細かい設定\n",
    "#!sudo apt install git-lfs\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#モデルを軽量化\n",
    "def quantize_transform(model):\n",
    "    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    return model\n",
    "quantize_transform(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasetdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ja_qu_ad (/home/s16991/.cache/huggingface/datasets/SkelterLabsInc___ja_qu_ad/default/0.1.0/5847b2e2ab5e02de284395bb15f87f13eae8f6f6ff1f01e4ee9c5c0dcf8ef8eb)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f28b19a04a4cf1b33deaf81767985a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetdict = datasets.load_dataset(args['dataset'])\n",
    "datasetdict = datasetdict.flatten()\\\n",
    "            .rename_column('answers.text', 'answer')\\\n",
    "            .rename_column('answers.answer_start', 'answer_start')\\\n",
    "            .rename_column('answers.answer_type', 'answer_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer to official base line\n",
    "def preprocess_function(examples):\n",
    "        examples[\"question\"] = [\"[CLS]\"+examples[\"question\"][i] for i in range(len(examples[\"question\"]))]\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples['question' if pad_on_right else \"context\"],\n",
    "            examples['context' if pad_on_right else \"question\"],\n",
    "            padding = \"max_length\",\n",
    "            max_length=args['max_length'],\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': [],\n",
    "            'attention_mask': [],\n",
    "            'start_positions': [],\n",
    "            'end_positions': [],\n",
    "        }\n",
    "        for tokens, att_mask, type_ids, context, answer,question,start_char \\\n",
    "                in zip(tokenized_examples['input_ids'],\n",
    "                       tokenized_examples['attention_mask'],\n",
    "                       tokenized_examples['token_type_ids'],\n",
    "                       examples['context'],\n",
    "                       examples['answer'],\n",
    "                       examples['question'],\n",
    "                       examples['answer_start']):\n",
    "                  \n",
    "            sep_index = tokens.index(2)\n",
    "            type_ids = [0 if i+1<=sep_index else att_mask[i] for i in range(len(att_mask))]\n",
    "\n",
    "            answer = answer[0]\n",
    "            start_char = start_char[0]\n",
    "            offsets = get_offsets(tokens,context, tokenizer,\n",
    "                                  args[\"norm_form\"])\n",
    "            \n",
    "\n",
    "            ctx_start = tokens.index(2) + 1\n",
    "            answer_start_index = 0\n",
    "            answer_end_index = len(offsets) - 2\n",
    "            \n",
    "            while offsets[answer_start_index][0] < start_char:\n",
    "                answer_start_index += 1\n",
    "            while offsets[answer_end_index][1] > start_char + len(answer):\n",
    "                answer_end_index -= 1\n",
    "            answer_start_index += ctx_start\n",
    "            answer_end_index += ctx_start\n",
    "\n",
    "            span_inputs = {\n",
    "                'input_ids': tokens,\n",
    "                'attention_mask': att_mask,\n",
    "                'token_type_ids': type_ids,\n",
    "            }\n",
    "\n",
    "            for span, answer_idx in make_spans(\n",
    "                span_inputs,\n",
    "                question_len=ctx_start,\n",
    "                max_seq_len=args[\"max_length\"],\n",
    "                stride=args[\"doc_stride\"],\n",
    "                answer_start_position=answer_start_index,\n",
    "                answer_end_position=answer_end_index):\n",
    "                inputs['input_ids'].append(span['input_ids'])\n",
    "                inputs['attention_mask'].append(span['attention_mask'])\n",
    "                inputs['start_positions'].append(answer_idx[0])\n",
    "                inputs['end_positions'].append(answer_idx[1]) \n",
    "            \n",
    "        return inputs\n",
    "   \n",
    "def make_spans(\n",
    "    inputs: Dict[str, Union[int, List[int]]],\n",
    "    question_len: int,\n",
    "    max_seq_len: int,\n",
    "    stride: int,\n",
    "    answer_start_position: int = -1,\n",
    "    answer_end_position: int = -1,\n",
    ") -> Iterator[Tuple[Dict[str, List[int]], Tuple[int, int]]]:\n",
    "    input_len = len(inputs['input_ids'])\n",
    "    context_len = input_len - question_len\n",
    "\n",
    "    def make_value(input_list, i, padding=0):\n",
    "        context_end = min(max_seq_len - question_len, context_len - i)\n",
    "        pad_len = max_seq_len - question_len - context_end\n",
    "        val = input_list[:question_len]\n",
    "        val += input_list[question_len + i:question_len + i + context_end]\n",
    "        val[-1] = input_list[-1]\n",
    "        val += [padding] * pad_len\n",
    "        return val\n",
    "    for i in range(0, input_len - max_seq_len + stride, stride):\n",
    "        span = {key: make_value(val, i) for key, val in inputs.items()}\n",
    "        answer_start = answer_start_position - i\n",
    "        answer_end = answer_end_position - i\n",
    "        if answer_start < question_len or answer_end >= max_seq_len - 1:\n",
    "            answer_start = answer_end = 0\n",
    "        yield span, (answer_start, answer_end)    \n",
    "\n",
    "def get_offsets(input_ids: List[int],\n",
    "                context: str,\n",
    "                tokenizer: AutoTokenizer,\n",
    "                norm_form='NFKC') -> List[Tuple[int, int]]:\n",
    "    \n",
    "    cxt_start = input_ids.index(2) + 1\n",
    "    cxt_end = cxt_start + input_ids[cxt_start:].index(2)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[cxt_start:cxt_end])\n",
    "    tokens = [tok[2:] if tok.startswith('▁') else tok for tok in tokens]\n",
    "    whitespace = string.whitespace + '\\u3000'\n",
    "\n",
    "    # 1 . Make offsets of normalized context within the original context.\n",
    "    offsets_norm_context = []\n",
    "    norm_context = ''\n",
    "    for idx, char in enumerate(context):\n",
    "        norm_char = unicodedata.normalize(norm_form, char)\n",
    "        norm_context += norm_char\n",
    "        offsets_norm_context.extend([idx] * len(norm_char))\n",
    "    norm_context_org = unicodedata.normalize(norm_form, context)\n",
    "    assert norm_context == norm_context_org, \\\n",
    "        'Normalized contexts are not the same: ' \\\n",
    "        + f'{norm_context} != {norm_context_org}'\n",
    "    assert len(norm_context) == len(offsets_norm_context), \\\n",
    "        'Normalized contexts have different numbers of tokens: ' \\\n",
    "        + f'{len(norm_context)} != {len(offsets_norm_context)}'\n",
    "\n",
    "    # 2. Make offsets of tokens (input_ids) within the normalized context.\n",
    "    offsets_token = []\n",
    "    unk_pointer = None\n",
    "    cid = 0\n",
    "    tid = 0\n",
    "    while tid < len(tokens):\n",
    "        cur_token = tokens[tid]\n",
    "        if cur_token == tokenizer.unk_token:\n",
    "            unk_pointer = tid\n",
    "            offsets_token.append([cid, cid])\n",
    "            cid += 1\n",
    "        # elif norm_context[cid:cid + len(cur_token)] != cur_token:\n",
    "        #     # Wrong offsets of the previous UNK token\n",
    "        #     assert unk_pointer is not None, \\\n",
    "        #         'Normalized context and tokens are not matched'\n",
    "        #     prev_unk_expected = offsets_token[unk_pointer]\n",
    "        #     prev_unk_expected[1] += norm_context[prev_unk_expected[1] + 2:]\\\n",
    "        #         .index(tokens[unk_pointer + 1]) + 1\n",
    "        #     tid = unk_pointer\n",
    "        #     offsets_token = offsets_token[:tid] + [prev_unk_expected]\n",
    "        #     cid = prev_unk_expected[1] + 1\n",
    "        else:\n",
    "            start_pos = norm_context[cid:].index(cur_token)\n",
    "            if start_pos > 0 and tokens[tid - 1] == tokenizer.unk_token:\n",
    "                offsets_token[-1][1] += start_pos\n",
    "                cid += start_pos\n",
    "                start_pos = 0\n",
    "            offsets_token.append([cid, cid + len(cur_token) - 1])\n",
    "            cid += len(cur_token)\n",
    "            while cid < len(norm_context) and norm_context[cid] in whitespace:\n",
    "                offsets_token[-1][1] += 1\n",
    "                cid += 1\n",
    "        tid += 1\n",
    "    if tokens[-1] == tokenizer.unk_token:\n",
    "        offsets_token[-1][1] = len(norm_context) - 1\n",
    "    assert len(offsets_token) == len(tokens), \\\n",
    "        'The numbers of tokens and offsets are different'\n",
    "\n",
    "    offsets_mapping = [(offsets_norm_context[start], offsets_norm_context[end])\n",
    "                       for start, end in offsets_token]\n",
    "    return [(0, 0)] + offsets_mapping+[(0, 0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[4, 5242, 216, 1879, 1302, 11091, 63, 147, 19, 7, 27730, 10, 13134, 553, 6768, 11, 5943, 3017, 2, 9, 28889, 15, 58, 16154, 13726, 561, 7, 3875, 76, 25649, 687, 15, 3009, 851, 14, 15481, 16, 15, 112, 31, 16, 3824, 22, 31, 33, 61, 4485, 16, 15, 16980, 14, 25, 22, 52, 33, 14, 11, 7, 14122, 7, 21244, 7, 1047, 464, 27, 8, 5242, 15045, 1879, 1302, 11091, 63, 5905, 7, 1302, 1030, 10, 19365, 132, 1763, 12963, 8, 9, 3656, 6227, 3202, 15, 8967, 11, 2882, 1311, 1624, 139, 1311, 82, 87, 7, 373, 1311, 82, 69, 14, 11661, 2957, 1296, 27, 8, 8773, 1744, 22585, 2966, 1673, 126, 8236, 8, 9, 2800, 4459, 3113, 2089, 27730, 15, 7907, 356, 14689, 13, 9117, 16, 14, 8, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 9, 28889, 10, 14851, 11, 5964, 821, 3418, 95, 3017, 2, 9, 28889, 15, 58, 16154, 13726, 561, 7, 3875, 76, 25649, 687, 15, 3009, 851, 14, 15481, 16, 15, 112, 31, 16, 3824, 22, 31, 33, 61, 4485, 16, 15, 16980, 14, 25, 22, 52, 33, 14, 11, 7, 14122, 7, 21244, 7, 1047, 464, 27, 8, 5242, 15045, 1879, 1302, 11091, 63, 5905, 7, 1302, 1030, 10, 19365, 132, 1763, 12963, 8, 9, 3656, 6227, 3202, 15, 8967, 11, 2882, 1311, 1624, 139, 1311, 82, 87, 7, 373, 1311, 82, 69, 14, 11661, 2957, 1296, 27, 8, 8773, 1744, 22585, 2966, 1673, 126, 8236, 8, 9, 2800, 4459, 3113, 2089, 27730, 15, 7907, 356, 14689, 13, 9117, 16, 14, 8, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 9, 28889, 10, 6768, 1389, 16196, 11, 6173, 57, 3017, 2, 8773, 1744, 22585, 2966, 1673, 126, 7544, 10, 8618, 16, 24, 22, 24, 67, 37, 22069, 39, 157, 88, 3934, 10, 5415, 4789, 169, 39, 3545, 1364, 1307, 36, 1935, 14, 19, 6768, 34, 9113, 8, 7985, 16, 7, 13752, 1050, 385, 16451, 10, 28988, 4654, 39, 123, 31498, 155, 36, 12, 17232, 159, 7, 636, 17, 817, 96, 4954, 12527, 8, 4781, 1421, 22156, 8492, 388, 941, 2793, 88, 3386, 1507, 23175, 62, 1457, 1507, 15346, 10, 7186, 36, 477, 28154, 18, 6203, 21017, 8, 7089, 16, 7, 12404, 7480, 99, 9059, 141, 152, 1162, 10, 4064, 340, 39, 941, 2793, 88, 3386, 36, 10297, 7, 1416, 13647, 216, 4064, 828, 22553, 20458, 40, 8, 2936, 5511, 39, 2523, 13, 3153, 1507, 18206, 303, 12, 16515, 56, 1507, 22279, 213, 36, 120, 28154, 3754, 8, 48, 7299, 137, 39, 1187, 314, 2749, 10, 1237, 1507, 13839, 17, 12758, 1477, 36, 45, 3066, 1302, 1919, 16294, 19834, 8, 15419, 28, 4485, 223, 2376, 109, 883, 10130, 24971, 32, 6337, 7, 23828, 82, 28, 23, 9965, 10, 22196, 21, 27539, 8, 9, 23556, 27157, 15, 23556, 13, 0, 13, 27157, 7, 23556, 27157, 0, 14, 7, 284, 202, 987, 978, 1021, 7, 817, 2993, 25747, 764, 7, 12835, 288, 3482, 7, 13254, 644, 129, 7, 1815, 261, 8553, 315, 7, 10429, 664, 1687, 873, 45, 2631, 5016, 12499, 25649, 17, 20267, 7, 6676, 32, 6768, 22396, 40, 8, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 7985, 44, 636, 17, 817, 96, 4954, 9145, 10, 11, 11133, 3017, 2, 8773, 1744, 22585, 2966, 1673, 126, 7544, 10, 8618, 16, 24, 22, 24, 67, 37, 22069, 39, 157, 88, 3934, 10, 5415, 4789, 169, 39, 3545, 1364, 1307, 36, 1935, 14, 19, 6768, 34, 9113, 8, 7985, 16, 7, 13752, 1050, 385, 16451, 10, 28988, 4654, 39, 123, 31498, 155, 36, 12, 17232, 159, 7, 636, 17, 817, 96, 4954, 12527, 8, 4781, 1421, 22156, 8492, 388, 941, 2793, 88, 3386, 1507, 23175, 62, 1457, 1507, 15346, 10, 7186, 36, 477, 28154, 18, 6203, 21017, 8, 7089, 16, 7, 12404, 7480, 99, 9059, 141, 152, 1162, 10, 4064, 340, 39, 941, 2793, 88, 3386, 36, 10297, 7, 1416, 13647, 216, 4064, 828, 22553, 20458, 40, 8, 2936, 5511, 39, 2523, 13, 3153, 1507, 18206, 303, 12, 16515, 56, 1507, 22279, 213, 36, 120, 28154, 3754, 8, 48, 7299, 137, 39, 1187, 314, 2749, 10, 1237, 1507, 13839, 17, 12758, 1477, 36, 45, 3066, 1302, 1919, 16294, 19834, 8, 15419, 28, 4485, 223, 2376, 109, 883, 10130, 24971, 32, 6337, 7, 23828, 82, 28, 23, 9965, 10, 22196, 21, 27539, 8, 9, 23556, 27157, 15, 23556, 13, 0, 13, 27157, 7, 23556, 27157, 0, 14, 7, 284, 202, 987, 978, 1021, 7, 817, 2993, 25747, 764, 7, 12835, 288, 3482, 7, 13254, 644, 129, 7, 1815, 261, 8553, 315, 7, 10429, 664, 1687, 873, 45, 2631, 5016, 12499, 25649, 17, 20267, 7, 6676, 32, 6768, 22396, 40, 8, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 9, 4481, 3028, 18559, 141, 152, 1162, 10, 4064, 72, 258, 1160, 1059, 3017, 2, 8773, 1744, 22585, 2966, 1673, 126, 7544, 10, 8618, 16, 24, 22, 24, 67, 37, 22069, 39, 157, 88, 3934, 10, 5415, 4789, 169, 39, 3545, 1364, 1307, 36, 1935, 14, 19, 6768, 34, 9113, 8, 7985, 16, 7, 13752, 1050, 385, 16451, 10, 28988, 4654, 39, 123, 31498, 155, 36, 12, 17232, 159, 7, 636, 17, 817, 96, 4954, 12527, 8, 4781, 1421, 22156, 8492, 388, 941, 2793, 88, 3386, 1507, 23175, 62, 1457, 1507, 15346, 10, 7186, 36, 477, 28154, 18, 6203, 21017, 8, 7089, 16, 7, 12404, 7480, 99, 9059, 141, 152, 1162, 10, 4064, 340, 39, 941, 2793, 88, 3386, 36, 10297, 7, 1416, 13647, 216, 4064, 828, 22553, 20458, 40, 8, 2936, 5511, 39, 2523, 13, 3153, 1507, 18206, 303, 12, 16515, 56, 1507, 22279, 213, 36, 120, 28154, 3754, 8, 48, 7299, 137, 39, 1187, 314, 2749, 10, 1237, 1507, 13839, 17, 12758, 1477, 36, 45, 3066, 1302, 1919, 16294, 19834, 8, 15419, 28, 4485, 223, 2376, 109, 883, 10130, 24971, 32, 6337, 7, 23828, 82, 28, 23, 9965, 10, 22196, 21, 27539, 8, 9, 23556, 27157, 15, 23556, 13, 0, 13, 27157, 7, 23556, 27157, 0, 14, 7, 284, 202, 987, 978, 1021, 7, 817, 2993, 25747, 764, 7, 12835, 288, 3482, 7, 13254, 644, 129, 7, 1815, 261, 8553, 315, 7, 10429, 664, 1687, 873, 45, 2631, 5016, 12499, 25649, 17, 20267, 7, 6676, 32, 6768, 22396, 40, 8, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 9, 28889, 21574, 27, 39, 941, 2793, 88, 3386, 1874, 15346, 10, 7186, 1874, 13839, 17, 12758, 1477, 36, 708, 7, 1016, 10736, 3484, 258, 1160, 1059, 3017, 2, 8773, 1744, 22585, 2966, 1673, 126, 7544, 10, 8618, 16, 24, 22, 24, 67, 37, 22069, 39, 157, 88, 3934, 10, 5415, 4789, 169, 39, 3545, 1364, 1307, 36, 1935, 14, 19, 6768, 34, 9113, 8, 7985, 16, 7, 13752, 1050, 385, 16451, 10, 28988, 4654, 39, 123, 31498, 155, 36, 12, 17232, 159, 7, 636, 17, 817, 96, 4954, 12527, 8, 4781, 1421, 22156, 8492, 388, 941, 2793, 88, 3386, 1507, 23175, 62, 1457, 1507, 15346, 10, 7186, 36, 477, 28154, 18, 6203, 21017, 8, 7089, 16, 7, 12404, 7480, 99, 9059, 141, 152, 1162, 10, 4064, 340, 39, 941, 2793, 88, 3386, 36, 10297, 7, 1416, 13647, 216, 4064, 828, 22553, 20458, 40, 8, 2936, 5511, 39, 2523, 13, 3153, 1507, 18206, 303, 12, 16515, 56, 1507, 22279, 213, 36, 120, 28154, 3754, 8, 48, 7299, 137, 39, 1187, 314, 2749, 10, 1237, 1507, 13839, 17, 12758, 1477, 36, 45, 3066, 1302, 1919, 16294, 19834, 8, 15419, 28, 4485, 223, 2376, 109, 883, 10130, 24971, 32, 6337, 7, 23828, 82, 28, 23, 9965, 10, 22196, 21, 27539, 8, 9, 23556, 27157, 15, 23556, 13, 0, 13, 27157, 7, 23556, 27157, 0, 14, 7, 284, 202, 987, 978, 1021, 7, 817, 2993, 25747, 764, 7, 12835, 288, 3482, 7, 13254, 644, 129, 7, 1815, 261, 8553, 315, 7, 10429, 664, 1687, 873, 45, 2631, 5016, 12499, 25649, 17, 20267, 7, 6676, 32, 6768, 22396, 40, 8, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 9, 28889, 12, 1354, 502, 12, 1359, 1713, 1913, 8212, 7, 19141, 1302, 24842, 455, 11177, 3017, 2, 6773, 16, 7, 17031, 18041, 752, 15, 373, 17031, 18041, 755, 14, 24950, 8, 9, 180, 979, 2825, 1354, 502, 12, 1359, 428, 19192, 1913, 75, 7, 796, 668, 208, 92, 1582, 32, 7, 1302, 16672, 10, 18, 455, 922, 4472, 10, 13199, 17, 28630, 18345, 1038, 45, 54, 8, 11497, 7, 3553, 1817, 6587, 30796, 12864, 927, 855, 1302, 11041, 35, 3033, 7, 25649, 349, 23, 265, 502, 2386, 0, 7981, 21, 45, 3882, 8630, 71, 24562, 18, 12404, 1758, 19141, 331, 35, 8, 5999, 13924, 11, 5168, 8808, 1578, 2986, 1038, 4181, 2389, 4472, 292, 25652, 56, 8, 2687, 8509, 455, 6582, 3888, 18249, 988, 17, 8408, 101, 10092, 7, 5075, 28126, 10, 4134, 17, 35, 60, 6656, 7827, 8, 4491, 16, 31, 22, 7, 14401, 10, 23942, 16, 5544, 5680, 74, 18041, 3454, 18, 37, 16, 19, 1520, 8, 9, 7178, 3967, 2258, 755, 15, 373, 636, 183, 14, 18, 5435, 5602, 7, 1302, 4380, 30456, 97, 7, 29089, 72, 8, 2084, 22, 7, 27259, 16800, 19, 4519, 31036, 3880, 2981, 636, 62, 5153, 17, 16452, 7, 402, 1279, 2692, 17544, 846, 12, 11616, 98, 30, 1898, 12097, 17990, 2979, 8, 77, 5153, 11, 25649, 10, 265, 3587, 6767, 3548, 375, 159, 7, 245, 39, 1025, 10, 4280, 169, 6575, 16, 14, 26, 39, 1362, 7458, 964, 169, 6275, 16, 61, 2731, 16, 14, 120, 17529, 160, 258, 2565, 65, 6824, 8991, 8, 77, 3587, 945, 7, 25649, 11, 988, 6582, 10, 15379, 7, 6016, 315, 30, 341, 30492, 1302, 4927, 1102, 8, 511, 46, 22, 7, 25649, 11, 636, 1744, 22585, 2966, 1673, 2410, 984, 620, 7, 3368, 18, 3661, 53, 8, 9, 2966, 1673, 5273, 7, 9800, 2843, 176, 1056, 7, 24980, 2258, 483, 499, 5078, 10, 17548, 7951, 4048, 17, 636, 23750, 10, 312, 1519, 1215, 7652, 6154, 19, 7, 312, 540, 10078, 7178, 2966, 5352, 75, 7, 2751, 341, 7178, 752, 440, 3368, 3679, 78, 8, 8773, 183, 15, 345, 13, 636, 23750, 14, 6457, 2966, 1673, 5273, 10501, 44, 26664, 8, 168, 8591, 748, 2800, 700, 984, 396, 23175, 62, 1457, 26, 941, 2793, 88, 3386, 45, 1935, 10, 3384, 19206, 7786, 54, 8, 2, 3, 3], [4, 138, 1025, 10, 4280, 36, 26, 39, 1362, 7458, 964, 36, 120, 17529, 160, 258, 2565, 30, 3585, 134, 28889, 1522, 1118, 17285, 124, 95, 9798, 18132, 4786, 11637, 23120, 3017, 2, 6773, 16, 7, 17031, 18041, 752, 15, 373, 17031, 18041, 755, 14, 24950, 8, 9, 180, 979, 2825, 1354, 502, 12, 1359, 428, 19192, 1913, 75, 7, 796, 668, 208, 92, 1582, 32, 7, 1302, 16672, 10, 18, 455, 922, 4472, 10, 13199, 17, 28630, 18345, 1038, 45, 54, 8, 11497, 7, 3553, 1817, 6587, 30796, 12864, 927, 855, 1302, 11041, 35, 3033, 7, 25649, 349, 23, 265, 502, 2386, 0, 7981, 21, 45, 3882, 8630, 71, 24562, 18, 12404, 1758, 19141, 331, 35, 8, 5999, 13924, 11, 5168, 8808, 1578, 2986, 1038, 4181, 2389, 4472, 292, 25652, 56, 8, 2687, 8509, 455, 6582, 3888, 18249, 988, 17, 8408, 101, 10092, 7, 5075, 28126, 10, 4134, 17, 35, 60, 6656, 7827, 8, 4491, 16, 31, 22, 7, 14401, 10, 23942, 16, 5544, 5680, 74, 18041, 3454, 18, 37, 16, 19, 1520, 8, 9, 7178, 3967, 2258, 755, 15, 373, 636, 183, 14, 18, 5435, 5602, 7, 1302, 4380, 30456, 97, 7, 29089, 72, 8, 2084, 22, 7, 27259, 16800, 19, 4519, 31036, 3880, 2981, 636, 62, 5153, 17, 16452, 7, 402, 1279, 2692, 17544, 846, 12, 11616, 98, 30, 1898, 12097, 17990, 2979, 8, 77, 5153, 11, 25649, 10, 265, 3587, 6767, 3548, 375, 159, 7, 245, 39, 1025, 10, 4280, 169, 6575, 16, 14, 26, 39, 1362, 7458, 964, 169, 6275, 16, 61, 2731, 16, 14, 120, 17529, 160, 258, 2565, 65, 6824, 8991, 8, 77, 3587, 945, 7, 25649, 11, 988, 6582, 10, 15379, 7, 6016, 315, 30, 341, 30492, 1302, 4927, 1102, 8, 511, 46, 22, 7, 25649, 11, 636, 1744, 22585, 2966, 1673, 2410, 984, 620, 7, 3368, 18, 3661, 53, 8, 9, 2966, 1673, 5273, 7, 9800, 2843, 176, 1056, 7, 24980, 2258, 483, 499, 5078, 10, 17548, 7951, 4048, 17, 636, 23750, 10, 312, 1519, 1215, 7652, 6154, 19, 7, 312, 540, 10078, 7178, 2966, 5352, 75, 7, 2751, 341, 7178, 752, 440, 3368, 3679, 78, 8, 8773, 183, 15, 345, 13, 636, 23750, 14, 6457, 2966, 1673, 5273, 10501, 44, 26664, 8, 168, 8591, 748, 2800, 700, 984, 396, 23175, 62, 1457, 26, 2], [4, 138, 1025, 10, 4280, 36, 26, 39, 1362, 7458, 964, 36, 120, 17529, 160, 258, 2565, 30, 3585, 134, 28889, 1522, 1118, 17285, 124, 95, 9798, 18132, 4786, 11637, 23120, 3017, 2, 10, 23942, 16, 5544, 5680, 74, 18041, 3454, 18, 37, 16, 19, 1520, 8, 9, 7178, 3967, 2258, 755, 15, 373, 636, 183, 14, 18, 5435, 5602, 7, 1302, 4380, 30456, 97, 7, 29089, 72, 8, 2084, 22, 7, 27259, 16800, 19, 4519, 31036, 3880, 2981, 636, 62, 5153, 17, 16452, 7, 402, 1279, 2692, 17544, 846, 12, 11616, 98, 30, 1898, 12097, 17990, 2979, 8, 77, 5153, 11, 25649, 10, 265, 3587, 6767, 3548, 375, 159, 7, 245, 39, 1025, 10, 4280, 169, 6575, 16, 14, 26, 39, 1362, 7458, 964, 169, 6275, 16, 61, 2731, 16, 14, 120, 17529, 160, 258, 2565, 65, 6824, 8991, 8, 77, 3587, 945, 7, 25649, 11, 988, 6582, 10, 15379, 7, 6016, 315, 30, 341, 30492, 1302, 4927, 1102, 8, 511, 46, 22, 7, 25649, 11, 636, 1744, 22585, 2966, 1673, 2410, 984, 620, 7, 3368, 18, 3661, 53, 8, 9, 2966, 1673, 5273, 7, 9800, 2843, 176, 1056, 7, 24980, 2258, 483, 499, 5078, 10, 17548, 7951, 4048, 17, 636, 23750, 10, 312, 1519, 1215, 7652, 6154, 19, 7, 312, 540, 10078, 7178, 2966, 5352, 75, 7, 2751, 341, 7178, 752, 440, 3368, 3679, 78, 8, 8773, 183, 15, 345, 13, 636, 23750, 14, 6457, 2966, 1673, 5273, 10501, 44, 26664, 8, 168, 8591, 748, 2800, 700, 984, 396, 23175, 62, 1457, 26, 941, 2793, 88, 3386, 45, 1935, 10, 3384, 19206, 7786, 54, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 138, 1025, 10, 4280, 36, 20, 39, 1362, 7458, 964, 36, 2828, 11, 11299, 12, 5556, 6772, 10, 3017, 2, 6773, 16, 7, 17031, 18041, 752, 15, 373, 17031, 18041, 755, 14, 24950, 8, 9, 180, 979, 2825, 1354, 502, 12, 1359, 428, 19192, 1913, 75, 7, 796, 668, 208, 92, 1582, 32, 7, 1302, 16672, 10, 18, 455, 922, 4472, 10, 13199, 17, 28630, 18345, 1038, 45, 54, 8, 11497, 7, 3553, 1817, 6587, 30796, 12864, 927, 855, 1302, 11041, 35, 3033, 7, 25649, 349, 23, 265, 502, 2386, 0, 7981, 21, 45, 3882, 8630, 71, 24562, 18, 12404, 1758, 19141, 331, 35, 8, 5999, 13924, 11, 5168, 8808, 1578, 2986, 1038, 4181, 2389, 4472, 292, 25652, 56, 8, 2687, 8509, 455, 6582, 3888, 18249, 988, 17, 8408, 101, 10092, 7, 5075, 28126, 10, 4134, 17, 35, 60, 6656, 7827, 8, 4491, 16, 31, 22, 7, 14401, 10, 23942, 16, 5544, 5680, 74, 18041, 3454, 18, 37, 16, 19, 1520, 8, 9, 7178, 3967, 2258, 755, 15, 373, 636, 183, 14, 18, 5435, 5602, 7, 1302, 4380, 30456, 97, 7, 29089, 72, 8, 2084, 22, 7, 27259, 16800, 19, 4519, 31036, 3880, 2981, 636, 62, 5153, 17, 16452, 7, 402, 1279, 2692, 17544, 846, 12, 11616, 98, 30, 1898, 12097, 17990, 2979, 8, 77, 5153, 11, 25649, 10, 265, 3587, 6767, 3548, 375, 159, 7, 245, 39, 1025, 10, 4280, 169, 6575, 16, 14, 26, 39, 1362, 7458, 964, 169, 6275, 16, 61, 2731, 16, 14, 120, 17529, 160, 258, 2565, 65, 6824, 8991, 8, 77, 3587, 945, 7, 25649, 11, 988, 6582, 10, 15379, 7, 6016, 315, 30, 341, 30492, 1302, 4927, 1102, 8, 511, 46, 22, 7, 25649, 11, 636, 1744, 22585, 2966, 1673, 2410, 984, 620, 7, 3368, 18, 3661, 53, 8, 9, 2966, 1673, 5273, 7, 9800, 2843, 176, 1056, 7, 24980, 2258, 483, 499, 5078, 10, 17548, 7951, 4048, 17, 636, 23750, 10, 312, 1519, 1215, 7652, 6154, 19, 7, 312, 540, 10078, 7178, 2966, 5352, 75, 7, 2751, 341, 7178, 752, 440, 3368, 3679, 78, 8, 8773, 183, 15, 345, 13, 636, 23750, 14, 6457, 2966, 1673, 5273, 10501, 44, 26664, 8, 168, 8591, 748, 2800, 700, 984, 396, 23175, 62, 1457, 26, 941, 2793, 88, 3386, 45, 1935, 10, 3384, 19206, 7786, 54, 8, 2], [4, 138, 157, 88, 3934, 10, 5415, 4789, 36, 17522, 8139, 8035, 28, 8035, 109, 3017, 2, 9, 16539, 7, 1186, 27, 25649, 11, 14401, 17, 12422, 30497, 40, 8368, 5126, 39, 19935, 629, 169, 39, 10953, 11098, 36, 17746, 14, 49, 8368, 23834, 12422, 11708, 7, 20132, 20355, 126, 90, 13279, 8, 444, 515, 23371, 9726, 4186, 12, 7, 634, 7, 4998, 16151, 20132, 10, 3935, 370, 17, 5917, 56, 577, 28, 10209, 19, 7, 22589, 10, 39, 3545, 1364, 1307, 169, 373, 4563, 9482, 1307, 14, 20355, 2410, 3210, 248, 49, 929, 17, 13915, 7, 1063, 30832, 58, 39, 3545, 1364, 1307, 36, 10, 636, 349, 17, 37, 22069, 39, 157, 88, 3934, 10, 5415, 4789, 36, 18, 1935, 15, 8618, 16, 24, 22, 24, 33, 2342, 22, 457, 33, 14, 7, 7098, 12, 25649, 10, 16196, 72, 8, 77, 39, 157, 88, 3934, 36, 11, 4874, 527, 1253, 268, 7, 5293, 26, 14350, 5944, 7658, 14501, 49, 659, 30, 7224, 8, 138, 157, 88, 3934, 36, 17, 11040, 37, 928, 39, 980, 33, 33, 1307, 36, 17, 37, 22069, 39, 9940, 4456, 20, 1113, 2278, 36, 18, 1935, 453, 7, 2533, 20, 8738, 55, 37, 7239, 10085, 1935, 8368, 258, 39, 0, 3934, 0, 129, 3934, 13107, 745, 1507, 12833, 20792, 629, 1507, 20274, 1925, 169, 12539, 375, 13848, 280, 14, 2704, 360, 1025, 17, 7570, 12, 7, 37, 7239, 1935, 49, 1067, 17, 6329, 268, 7, 108, 25, 1616, 9775, 6223, 1808, 1929, 16927, 546, 8, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'start_positions': [19, 78, 30, 64, 121, 181, 24, 211, 83, 235, 124], 'end_positions': [22, 80, 37, 71, 124, 184, 26, 214, 86, 237, 134]}\n"
     ]
    }
   ],
   "source": [
    "#テスト\n",
    "features = preprocess_function(datasetdict['train'][:10])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a359d9db8aa1464ebefebdb8eb25a7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28144/3725051835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#all data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenized_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasetdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasetdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenized_valid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasetdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasetdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenized_train_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenized_valid_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1969\u001b[0m                 \u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m                 \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m                 \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m             )\n\u001b[1;32m   1973\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m         }\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2339\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m                                 \u001b[0mcheck_same_num_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m                                 \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2342\u001b[0m                             )\n\u001b[1;32m   2343\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mNumExamplesMismatchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2218\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m                 \u001b[0;31m# Check if the function returns updated examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m                 )\n\u001b[1;32m   1912\u001b[0m                 \u001b[0;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m                 \u001b[0;31m# Return a standard dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28144/458210853.py\u001b[0m in \u001b[0;36mpreprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0manswer_end_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer_start_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstart_char\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0manswer_start_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer_end_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart_char\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#all data\n",
    "tokenized_train_dataset = datasetdict[\"train\"].map(preprocess_function, batched=True, remove_columns=datasetdict[\"train\"].column_names)\n",
    "tokenized_valid_dataset = datasetdict[\"validation\"].map(preprocess_function, batched=True, remove_columns=datasetdict[\"train\"].column_names)\n",
    "tokenized_train_dataset.set_format(type='torch')\n",
    "tokenized_valid_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/chiba/japanese-roberta-base_same_prepare into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "model_name = args['pretrained_model'].split(\"/\")[-1]\n",
    "note = args[\"note\"]\n",
    "data_collator = default_data_collator\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir= f\"./model/{model_name}_{note}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=args[\"lr\"],\n",
    "    per_device_train_batch_size= args[\"batch_size\"],\n",
    "    per_device_eval_batch_size= args[\"eval_batch_size\"],\n",
    "    num_train_epochs=args[\"epochs\"],\n",
    "    weight_decay=args[\"weight_decay\"],\n",
    "    push_to_hub=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"{model_name}_{note}\",\n",
    "    load_best_model_at_end = True,\n",
    "    eval_accumulation_steps = args[\"eval_accumulation_steps\"] #メモリ対策？\n",
    ")\n",
    "\n",
    "trainer =  Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 67123\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8391\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.14 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/s16991/JaQuAD/wandb/run-20220413_073232-25n9zk0f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cashunsukechiba/JaQuad/runs/25n9zk0f\" target=\"_blank\">japanese-roberta-base_same_prepare</a></strong> to <a href=\"https://wandb.ai/cashunsukechiba/JaQuad\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5715' max='8391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5715/8391 1:04:27 < 30:11, 1.48 it/s, Epoch 0.68/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(start_positions, end_positions, start_preds,\n",
    "                        end_preds):\n",
    "    start_overlap = np.maximum(start_positions, start_preds)\n",
    "    end_overlap = np.minimum(end_positions, end_preds)\n",
    "    overlap = np.maximum(end_overlap - start_overlap + 1, 0)\n",
    "    pred_token_count = np.maximum(end_preds - start_preds + 1, 0)\n",
    "    ground_token_count = np.maximum(end_positions - start_positions + 1, 0)\n",
    "\n",
    "\n",
    "    precision = torch.nan_to_num(overlap / pred_token_count, nan=0.)\n",
    "    recall = torch.nan_to_num(overlap / ground_token_count, nan=0.)\n",
    "    f1 = torch.nan_to_num(\n",
    "        2 * precision * recall / (precision + recall), nan=0.)\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "    \n",
    "def calculate_exact_match(start_positions, end_positions, start_preds,\n",
    "                            end_preds):\n",
    "    equal_start = (start_preds == start_positions)\n",
    "    equal_end = (end_preds == end_positions)\n",
    "    return (equal_start * equal_end).to('cpu').detach().numpy().astype(np.float32)\n",
    "\n",
    "def get_result(dataset = tokenized_valid_dataset):\n",
    "    results = trainer.predict(dataset)\n",
    "    start_preds = results.predictions[0]\n",
    "    end_preds = results.predictions[1]\n",
    "    start_positions = results.label_ids[0]\n",
    "    end_positions = results.label_ids[1]\n",
    "    start_pred= torch.from_numpy(start_preds).argmax(dim=-1).cpu().detach()\n",
    "    end_pred= torch.from_numpy(end_preds).argmax(dim=-1).cpu().detach()\n",
    "    start_position = torch.from_numpy(start_positions).cpu().detach()\n",
    "    end_position = torch.from_numpy(end_positions).cpu().detach()\n",
    "    return start_pred,end_pred,start_position,end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pred,end_pred,start_position,end_position = get_result(tokenized_valid_dataset)\n",
    "f1_metrics = calculate_f1_score(start_position,end_position,start_pred,end_pred)\n",
    "\n",
    "precision = f1_metrics['precision'].mean()\n",
    "recall = f1_metrics['recall'].mean()\n",
    "f1 = f1_metrics['f1'].mean()\n",
    "em = calculate_exact_match(start_position,end_position,start_pred,end_pred).mean()\n",
    "\n",
    "print(\"========== CV ==========\")\n",
    "print(\"f1:\",f1.item())\n",
    "print(\"em:\",em.item())\n",
    "\n",
    "wandb.log({\"f1\": f1, \"em\": em})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '『三つ目がとおる』を発表したのはいつ？'\n",
    "context = '\"大阪帝国大学附属医学専門部在学中の1946年1月1日に4コマ漫画『マアチャンの日記帳』(『少国民新聞』連載)で漫画家としてデビューした。1947年、酒井七馬原案の描き下ろし単行本『新寶島』がベストセラーとなり、大阪に赤本ブームを引き起こす。1950年より漫画雑誌に登場、『鉄腕アトム』『ジャングル大帝』『リボンの騎士』といったヒット作を次々と手がけた。1963年、自作をもとに日本初となる30分枠のテレビアニメシリーズ『鉄腕アトム』を制作、現代につながる日本のテレビアニメ制作に多大な影響を及ぼした。1970年代には『ブラック・ジャック』『三つ目がとおる』『ブッダ』などのヒット作を発表。また晩年にも『陽だまりの樹』『アドルフに告ぐ』など青年漫画においても傑作を生み出す。デビューから1989年の死去まで第一線で作品を発表し続け、存命中から「マンガの神様」と評された。藤子不二雄(藤子・F・不二雄、藤子不二雄A)、石ノ森章太郎、赤塚不二夫、横山光輝、水野英子、矢代まさこ、萩尾望都などをはじめ数多くの人間が手塚に影響を受け、接触し漫画家を志した。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most likely beginning of answer with the argmax of the score.\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "\n",
    "answer_end = torch.argmax(answer_end_scores) -1\n",
    "# Get the most likely end of answer with the argmax of the score.\n",
    "# 1 is added to `answer_end` because the index pointed by score is inclusive.\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
