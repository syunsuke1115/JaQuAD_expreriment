{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robertaのノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import unicodedata\n",
    "from typing import Any, Dict, Iterator, List, Tuple, Union\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "import transformers \n",
    "from transformers import AutoTokenizer,AlbertTokenizer,BertJapaneseTokenizer,T5Tokenizer\n",
    "from transformers import AutoModelForQuestionAnswering,TrainingArguments,Trainer,ElectraForMaskedLM,RobertaForMaskedLM,AutoModel\n",
    "from transformers import default_data_collator\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ここを変更する\n",
    "args = {\n",
    "    'random_seed': 42,\n",
    "    'pretrained_model': 'rinna/japanese-roberta-base',\n",
    "    'pretrained_tokenizer': 'rinna/japanese-roberta-base',\n",
    "    'batch_size': 8, \n",
    "    \"eval_batch_size\":8,\n",
    "    'lr': 2e-5, \n",
    "    'max_length': 384,  \n",
    "    'doc_stride': 128,  \n",
    "    'epochs': 1,  \n",
    "    'dataset': 'SkelterLabsInc/JaQuAD',\n",
    "    'optimizer': 'AdamW',\n",
    "    'norm_form': 'NFKC',\n",
    "    'weight_decay': 0.01,  \n",
    "    'lr_scheduler': 'warmup_lin',\n",
    "    'warmup_ratio': 0.1,\n",
    "    \"eval_accumulation_steps\":10,\n",
    "    'cpu_workers': os.cpu_count(),\n",
    "    'note':\"same_prepare\",\n",
    "}\n",
    "args\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "            args[\"pretrained_tokenizer\"]\n",
    "            if args[\"pretrained_tokenizer\"] else\n",
    "            args[\"pretrained_model\"])\n",
    "# tokenizer.do_lower_case = True\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(args['pretrained_model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed値を固定\n",
    "def set_seed(seed =42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "set_seed(seed=args[\"random_seed\"])\n",
    "\n",
    "#wandb\n",
    "import wandb\n",
    "wandb.login()\n",
    "os.environ[\"WANDB_PROJECT\"] = \"JaQuad\"\n",
    "\n",
    "#細かい設定\n",
    "#!sudo apt install git-lfs\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#モデルを軽量化\n",
    "def quantize_transform(model):\n",
    "    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    return model\n",
    "quantize_transform(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasetdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetdict = datasets.load_dataset(args['dataset'])\n",
    "datasetdict = datasetdict.flatten()\\\n",
    "            .rename_column('answers.text', 'answer')\\\n",
    "            .rename_column('answers.answer_start', 'answer_start')\\\n",
    "            .rename_column('answers.answer_type', 'answer_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer to official base line\n",
    "def preprocess_function(examples):\n",
    "        examples[\"question\"] = [\"[CLS]\"+examples[\"question\"][i] for i in range(len(examples[\"question\"]))]\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples['question' if pad_on_right else \"context\"],\n",
    "            examples['context' if pad_on_right else \"question\"],\n",
    "            padding = \"max_length\",\n",
    "            max_length=args['max_length'],\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': [],\n",
    "            'attention_mask': [],\n",
    "            'start_positions': [],\n",
    "            'end_positions': [],\n",
    "        }\n",
    "        for tokens, att_mask, type_ids, context, answer,question,start_char \\\n",
    "                in zip(tokenized_examples['input_ids'],\n",
    "                       tokenized_examples['attention_mask'],\n",
    "                       tokenized_examples['token_type_ids'],\n",
    "                       examples['context'],\n",
    "                       examples['answer'],\n",
    "                       examples['question'],\n",
    "                       examples['answer_start']):\n",
    "                  \n",
    "            sep_index = tokens.index(2)\n",
    "            type_ids = [0 if i+1<=sep_index else att_mask[i] for i in range(len(att_mask))]\n",
    "\n",
    "            answer = answer[0]\n",
    "            start_char = start_char[0]\n",
    "            offsets = get_offsets(tokens,context, tokenizer,\n",
    "                                  args[\"norm_form\"])\n",
    "            \n",
    "\n",
    "            ctx_start = tokens.index(2) + 1\n",
    "            answer_start_index = 0\n",
    "            answer_end_index = len(offsets) - 2\n",
    "            \n",
    "            while offsets[answer_start_index][0] < start_char:\n",
    "                answer_start_index += 1\n",
    "            while offsets[answer_end_index][1] > start_char + len(answer):\n",
    "                answer_end_index -= 1\n",
    "            answer_start_index += ctx_start\n",
    "            answer_end_index += ctx_start\n",
    "\n",
    "            span_inputs = {\n",
    "                'input_ids': tokens,\n",
    "                'attention_mask': att_mask,\n",
    "                'token_type_ids': type_ids,\n",
    "            }\n",
    "\n",
    "            for span, answer_idx in make_spans(\n",
    "                span_inputs,\n",
    "                question_len=ctx_start,\n",
    "                max_seq_len=args[\"max_length\"],\n",
    "                stride=args[\"doc_stride\"],\n",
    "                answer_start_position=answer_start_index,\n",
    "                answer_end_position=answer_end_index):\n",
    "                inputs['input_ids'].append(span['input_ids'])\n",
    "                inputs['attention_mask'].append(span['attention_mask'])\n",
    "                inputs['start_positions'].append(answer_idx[0])\n",
    "                inputs['end_positions'].append(answer_idx[1]) \n",
    "            \n",
    "        return inputs\n",
    "   \n",
    "def make_spans(\n",
    "    inputs: Dict[str, Union[int, List[int]]],\n",
    "    question_len: int,\n",
    "    max_seq_len: int,\n",
    "    stride: int,\n",
    "    answer_start_position: int = -1,\n",
    "    answer_end_position: int = -1,\n",
    ") -> Iterator[Tuple[Dict[str, List[int]], Tuple[int, int]]]:\n",
    "    input_len = len(inputs['input_ids'])\n",
    "    context_len = input_len - question_len\n",
    "\n",
    "    def make_value(input_list, i, padding=0):\n",
    "        context_end = min(max_seq_len - question_len, context_len - i)\n",
    "        pad_len = max_seq_len - question_len - context_end\n",
    "        val = input_list[:question_len]\n",
    "        val += input_list[question_len + i:question_len + i + context_end]\n",
    "        val[-1] = input_list[-1]\n",
    "        val += [padding] * pad_len\n",
    "        return val\n",
    "    for i in range(0, input_len - max_seq_len + stride, stride):\n",
    "        span = {key: make_value(val, i) for key, val in inputs.items()}\n",
    "        answer_start = answer_start_position - i\n",
    "        answer_end = answer_end_position - i\n",
    "        if answer_start < question_len or answer_end >= max_seq_len - 1:\n",
    "            answer_start = answer_end = 0\n",
    "        yield span, (answer_start, answer_end)    \n",
    "\n",
    "def get_offsets(input_ids: List[int],\n",
    "                context: str,\n",
    "                tokenizer: AutoTokenizer,\n",
    "                norm_form='NFKC') -> List[Tuple[int, int]]:\n",
    "    \n",
    "    cxt_start = input_ids.index(2) + 1\n",
    "    cxt_end = cxt_start + input_ids[cxt_start:].index(2)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[cxt_start:cxt_end])\n",
    "    tokens = [tok[2:] if tok.startswith('▁') else tok for tok in tokens]\n",
    "    whitespace = string.whitespace + '\\u3000'\n",
    "\n",
    "    # 1 . Make offsets of normalized context within the original context.\n",
    "    offsets_norm_context = []\n",
    "    norm_context = ''\n",
    "    for idx, char in enumerate(context):\n",
    "        norm_char = unicodedata.normalize(norm_form, char)\n",
    "        norm_context += norm_char\n",
    "        offsets_norm_context.extend([idx] * len(norm_char))\n",
    "    norm_context_org = unicodedata.normalize(norm_form, context)\n",
    "    assert norm_context == norm_context_org, \\\n",
    "        'Normalized contexts are not the same: ' \\\n",
    "        + f'{norm_context} != {norm_context_org}'\n",
    "    assert len(norm_context) == len(offsets_norm_context), \\\n",
    "        'Normalized contexts have different numbers of tokens: ' \\\n",
    "        + f'{len(norm_context)} != {len(offsets_norm_context)}'\n",
    "\n",
    "    # 2. Make offsets of tokens (input_ids) within the normalized context.\n",
    "    offsets_token = []\n",
    "    unk_pointer = None\n",
    "    cid = 0\n",
    "    tid = 0\n",
    "    while tid < len(tokens):\n",
    "        cur_token = tokens[tid]\n",
    "        if cur_token == tokenizer.unk_token:\n",
    "            unk_pointer = tid\n",
    "            offsets_token.append([cid, cid])\n",
    "            cid += 1\n",
    "        # elif norm_context[cid:cid + len(cur_token)] != cur_token:\n",
    "        #     # Wrong offsets of the previous UNK token\n",
    "        #     assert unk_pointer is not None, \\\n",
    "        #         'Normalized context and tokens are not matched'\n",
    "        #     prev_unk_expected = offsets_token[unk_pointer]\n",
    "        #     prev_unk_expected[1] += norm_context[prev_unk_expected[1] + 2:]\\\n",
    "        #         .index(tokens[unk_pointer + 1]) + 1\n",
    "        #     tid = unk_pointer\n",
    "        #     offsets_token = offsets_token[:tid] + [prev_unk_expected]\n",
    "        #     cid = prev_unk_expected[1] + 1\n",
    "        else:\n",
    "            start_pos = norm_context[cid:].index(cur_token)\n",
    "            if start_pos > 0 and tokens[tid - 1] == tokenizer.unk_token:\n",
    "                offsets_token[-1][1] += start_pos\n",
    "                cid += start_pos\n",
    "                start_pos = 0\n",
    "            offsets_token.append([cid, cid + len(cur_token) - 1])\n",
    "            cid += len(cur_token)\n",
    "            while cid < len(norm_context) and norm_context[cid] in whitespace:\n",
    "                offsets_token[-1][1] += 1\n",
    "                cid += 1\n",
    "        tid += 1\n",
    "    if tokens[-1] == tokenizer.unk_token:\n",
    "        offsets_token[-1][1] = len(norm_context) - 1\n",
    "    assert len(offsets_token) == len(tokens), \\\n",
    "        'The numbers of tokens and offsets are different'\n",
    "\n",
    "    offsets_mapping = [(offsets_norm_context[start], offsets_norm_context[end])\n",
    "                       for start, end in offsets_token]\n",
    "    return [(0, 0)] + offsets_mapping+[(0, 0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テスト\n",
    "features = preprocess_function(datasetdict['train'][:10])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all data\n",
    "tokenized_train_dataset = datasetdict[\"train\"].map(preprocess_function, batched=True, remove_columns=datasetdict[\"train\"].column_names)\n",
    "tokenized_valid_dataset = datasetdict[\"validation\"].map(preprocess_function, batched=True, remove_columns=datasetdict[\"train\"].column_names)\n",
    "tokenized_train_dataset.set_format(type='torch')\n",
    "tokenized_valid_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = args['pretrained_model'].split(\"/\")[-1]\n",
    "note = args[\"note\"]\n",
    "data_collator = default_data_collator\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir= f\"./model/{model_name}_{note}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=args[\"lr\"],\n",
    "    per_device_train_batch_size= args[\"batch_size\"],\n",
    "    per_device_eval_batch_size= args[\"eval_batch_size\"],\n",
    "    num_train_epochs=args[\"epochs\"],\n",
    "    weight_decay=args[\"weight_decay\"],\n",
    "    push_to_hub=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"{model_name}_{note}\",\n",
    "    load_best_model_at_end = True,\n",
    "    eval_accumulation_steps = args[\"eval_accumulation_steps\"] #メモリ対策？\n",
    ")\n",
    "\n",
    "trainer =  Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(start_positions, end_positions, start_preds,\n",
    "                        end_preds):\n",
    "    start_overlap = np.maximum(start_positions, start_preds)\n",
    "    end_overlap = np.minimum(end_positions, end_preds)\n",
    "    overlap = np.maximum(end_overlap - start_overlap + 1, 0)\n",
    "    pred_token_count = np.maximum(end_preds - start_preds + 1, 0)\n",
    "    ground_token_count = np.maximum(end_positions - start_positions + 1, 0)\n",
    "\n",
    "\n",
    "    precision = torch.nan_to_num(overlap / pred_token_count, nan=0.)\n",
    "    recall = torch.nan_to_num(overlap / ground_token_count, nan=0.)\n",
    "    f1 = torch.nan_to_num(\n",
    "        2 * precision * recall / (precision + recall), nan=0.)\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "    \n",
    "def calculate_exact_match(start_positions, end_positions, start_preds,\n",
    "                            end_preds):\n",
    "    equal_start = (start_preds == start_positions)\n",
    "    equal_end = (end_preds == end_positions)\n",
    "    return (equal_start * equal_end).to('cpu').detach().numpy().astype(np.float32)\n",
    "\n",
    "def get_result(dataset = tokenized_valid_dataset):\n",
    "    results = trainer.predict(dataset)\n",
    "    start_preds = results.predictions[0]\n",
    "    end_preds = results.predictions[1]\n",
    "    start_positions = results.label_ids[0]\n",
    "    end_positions = results.label_ids[1]\n",
    "    start_pred= torch.from_numpy(start_preds).argmax(dim=-1).cpu().detach()\n",
    "    end_pred= torch.from_numpy(end_preds).argmax(dim=-1).cpu().detach()\n",
    "    start_position = torch.from_numpy(start_positions).cpu().detach()\n",
    "    end_position = torch.from_numpy(end_positions).cpu().detach()\n",
    "    return start_pred,end_pred,start_position,end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pred,end_pred,start_position,end_position = get_result(tokenized_valid_dataset)\n",
    "f1_metrics = calculate_f1_score(start_position,end_position,start_pred,end_pred)\n",
    "\n",
    "precision = f1_metrics['precision'].mean()\n",
    "recall = f1_metrics['recall'].mean()\n",
    "f1 = f1_metrics['f1'].mean()\n",
    "em = calculate_exact_match(start_position,end_position,start_pred,end_pred).mean()\n",
    "\n",
    "print(\"========== CV ==========\")\n",
    "print(\"f1:\",f1.item())\n",
    "print(\"em:\",em.item())\n",
    "\n",
    "wandb.log({\"f1\": f1, \"em\": em})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '『三つ目がとおる』を発表したのはいつ？'\n",
    "context = '\"大阪帝国大学附属医学専門部在学中の1946年1月1日に4コマ漫画『マアチャンの日記帳』(『少国民新聞』連載)で漫画家としてデビューした。1947年、酒井七馬原案の描き下ろし単行本『新寶島』がベストセラーとなり、大阪に赤本ブームを引き起こす。1950年より漫画雑誌に登場、『鉄腕アトム』『ジャングル大帝』『リボンの騎士』といったヒット作を次々と手がけた。1963年、自作をもとに日本初となる30分枠のテレビアニメシリーズ『鉄腕アトム』を制作、現代につながる日本のテレビアニメ制作に多大な影響を及ぼした。1970年代には『ブラック・ジャック』『三つ目がとおる』『ブッダ』などのヒット作を発表。また晩年にも『陽だまりの樹』『アドルフに告ぐ』など青年漫画においても傑作を生み出す。デビューから1989年の死去まで第一線で作品を発表し続け、存命中から「マンガの神様」と評された。藤子不二雄(藤子・F・不二雄、藤子不二雄A)、石ノ森章太郎、赤塚不二夫、横山光輝、水野英子、矢代まさこ、萩尾望都などをはじめ数多くの人間が手塚に影響を受け、接触し漫画家を志した。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most likely beginning of answer with the argmax of the score.\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "\n",
    "answer_end = torch.argmax(answer_end_scores) -1\n",
    "# Get the most likely end of answer with the argmax of the score.\n",
    "# 1 is added to `answer_end` because the index pointed by score is inclusive.\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
