{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELECTRAのノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import unicodedata\n",
    "from typing import Any, Dict, Iterator, List, Tuple, Union\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "import transformers \n",
    "from transformers import AutoTokenizer,AlbertTokenizer,BertJapaneseTokenizer,BertJapaneseTokenizer,T5Tokenizer\n",
    "from transformers import AutoModelForQuestionAnswering,TrainingArguments,Trainer,ElectraForMaskedLM,RobertaForMaskedLM,AutoModel\n",
    "from transformers import default_data_collator\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ここを変更する\n",
    "args = {\n",
    "    'random_seed': 42,\n",
    "    'pretrained_model': './model/bert-base-japanese-whole-word-masking_test/checkpoint-16781',\n",
    "    'pretrained_tokenizer': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "    'batch_size': 16, \n",
    "    \"eval_batch_size\":16,\n",
    "    'lr': 2e-5, \n",
    "    'max_length': 384,  \n",
    "    'doc_stride': 128,  \n",
    "    'epochs': 3,  \n",
    "    'dataset': 'SkelterLabsInc/JaQuAD',\n",
    "    'optimizer': 'AdamW',\n",
    "    'norm_form': 'NFKC',\n",
    "    'weight_decay': 0.01,  \n",
    "    'lr_scheduler': 'warmup_lin',\n",
    "    'warmup_ratio': 0.1,\n",
    "    \"eval_accumulation_steps\":10,\n",
    "    'cpu_workers': os.cpu_count(),\n",
    "    'note':\"same_prepare\",\n",
    "}\n",
    "args\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "            args[\"pretrained_tokenizer\"]\n",
    "            if args[\"pretrained_tokenizer\"] else\n",
    "            args[\"pretrained_model\"])\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(args['pretrained_model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed値を固定\n",
    "def set_seed(seed =42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "set_seed(seed=args[\"random_seed\"])\n",
    "\n",
    "#wandb\n",
    "import wandb\n",
    "wandb.login()\n",
    "os.environ[\"WANDB_PROJECT\"] = \"JaQuad\"\n",
    "\n",
    "#細かい設定\n",
    "#!sudo apt install git-lfs\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#モデルを軽量化\n",
    "def quantize_transform(model):\n",
    "    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    return model\n",
    "quantize_transform(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasetdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetdict = datasets.load_dataset(args['dataset'])\n",
    "datasetdict = datasetdict.flatten()\\\n",
    "            .rename_column('answers.text', 'answer')\\\n",
    "            .rename_column('answers.answer_start', 'answer_start')\\\n",
    "            .rename_column('answers.answer_type', 'answer_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer to official base line\n",
    "def preprocess_function(examples):\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples['question' if pad_on_right else \"context\"],\n",
    "            examples['context' if pad_on_right else \"question\"],\n",
    "            return_overflowing_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': [],\n",
    "            'attention_mask': [],\n",
    "            'start_positions': [],\n",
    "            'end_positions': [],\n",
    "        }\n",
    "        for tokens, att_mask, type_ids, context, answer,question,start_char \\\n",
    "                in zip(tokenized_examples['input_ids'],\n",
    "                       tokenized_examples['attention_mask'],\n",
    "                       tokenized_examples['token_type_ids'],\n",
    "                       examples['context'],\n",
    "                       examples['answer'],\n",
    "                       examples['question'],\n",
    "                       examples['answer_start']):\n",
    "\n",
    "            answer = answer[0]\n",
    "            start_char = start_char[0]\n",
    "            offsets = get_offsets(tokens, context, tokenizer,\n",
    "                                  args[\"norm_form\"])\n",
    "            \n",
    "\n",
    "            ctx_start = tokens.index(tokenizer.sep_token_id) + 1\n",
    "            answer_start_index = 0\n",
    "            answer_end_index = len(offsets) - 2\n",
    "            \n",
    "            while offsets[answer_start_index][0] < start_char:\n",
    "                answer_start_index += 1\n",
    "            while offsets[answer_end_index][1] > start_char + len(answer):\n",
    "                answer_end_index -= 1\n",
    "            answer_start_index += ctx_start\n",
    "            answer_end_index += ctx_start\n",
    "\n",
    "            span_inputs = {\n",
    "                'input_ids': tokens,\n",
    "                'attention_mask': att_mask,\n",
    "                'token_type_ids': type_ids,\n",
    "            }\n",
    "\n",
    "            for span, answer_idx in make_spans(\n",
    "                span_inputs,\n",
    "                question_len=ctx_start,\n",
    "                max_seq_len=args[\"max_length\"],\n",
    "                stride=args[\"doc_stride\"],\n",
    "                answer_start_position=answer_start_index,\n",
    "                answer_end_position=answer_end_index):\n",
    "                inputs['input_ids'].append(span['input_ids'])\n",
    "                inputs['attention_mask'].append(span['attention_mask'])\n",
    "                inputs['start_positions'].append(answer_idx[0])\n",
    "                inputs['end_positions'].append(answer_idx[1]) \n",
    "            \n",
    "        return inputs\n",
    "   \n",
    "def make_spans(\n",
    "    inputs: Dict[str, Union[int, List[int]]],\n",
    "    question_len: int,\n",
    "    max_seq_len: int,\n",
    "    stride: int,\n",
    "    answer_start_position: int = -1,\n",
    "    answer_end_position: int = -1,\n",
    ") -> Iterator[Tuple[Dict[str, List[int]], Tuple[int, int]]]:\n",
    "    input_len = len(inputs['input_ids'])\n",
    "    context_len = input_len - question_len\n",
    "\n",
    "    def make_value(input_list, i, padding=0):\n",
    "        context_end = min(max_seq_len - question_len, context_len - i)\n",
    "        pad_len = max_seq_len - question_len - context_end\n",
    "        val = input_list[:question_len]\n",
    "        val += input_list[question_len + i:question_len + i + context_end]\n",
    "        val[-1] = input_list[-1]\n",
    "        val += [padding] * pad_len\n",
    "        return val\n",
    "    for i in range(0, input_len - max_seq_len + stride, stride):\n",
    "        span = {key: make_value(val, i) for key, val in inputs.items()}\n",
    "        answer_start = answer_start_position - i\n",
    "        answer_end = answer_end_position - i\n",
    "        if answer_start < question_len or answer_end >= max_seq_len - 1:\n",
    "            answer_start = answer_end = 0\n",
    "        yield span, (answer_start, answer_end)    \n",
    "\n",
    "def get_offsets(input_ids: List[int],\n",
    "                context: str,\n",
    "                tokenizer: AutoTokenizer,\n",
    "                norm_form='NFKC') -> List[Tuple[int, int]]:\n",
    "    \n",
    "    cxt_start = input_ids.index(tokenizer.sep_token_id) + 1\n",
    "    cxt_end = cxt_start + input_ids[cxt_start:].index(tokenizer.sep_token_id)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[cxt_start:cxt_end])\n",
    "    tokens = [tok[2:] if tok.startswith('##') else tok for tok in tokens]\n",
    "    whitespace = string.whitespace + '\\u3000'\n",
    "\n",
    "    # 1 . Make offsets of normalized context within the original context.\n",
    "    offsets_norm_context = []\n",
    "    norm_context = ''\n",
    "    for idx, char in enumerate(context):\n",
    "        norm_char = unicodedata.normalize(norm_form, char)\n",
    "        norm_context += norm_char\n",
    "        offsets_norm_context.extend([idx] * len(norm_char))\n",
    "    norm_context_org = unicodedata.normalize(norm_form, context)\n",
    "    assert norm_context == norm_context_org, \\\n",
    "        'Normalized contexts are not the same: ' \\\n",
    "        + f'{norm_context} != {norm_context_org}'\n",
    "    assert len(norm_context) == len(offsets_norm_context), \\\n",
    "        'Normalized contexts have different numbers of tokens: ' \\\n",
    "        + f'{len(norm_context)} != {len(offsets_norm_context)}'\n",
    "\n",
    "    # 2. Make offsets of tokens (input_ids) within the normalized context.\n",
    "    offsets_token = []\n",
    "    unk_pointer = None\n",
    "    cid = 0\n",
    "    tid = 0\n",
    "    while tid < len(tokens):\n",
    "        cur_token = tokens[tid]\n",
    "        if cur_token == tokenizer.unk_token:\n",
    "            unk_pointer = tid\n",
    "            offsets_token.append([cid, cid])\n",
    "            cid += 1\n",
    "        elif norm_context[cid:cid + len(cur_token)] != cur_token:\n",
    "            # Wrong offsets of the previous UNK token\n",
    "            assert unk_pointer is not None, \\\n",
    "                'Normalized context and tokens are not matched'\n",
    "            prev_unk_expected = offsets_token[unk_pointer]\n",
    "            prev_unk_expected[1] += norm_context[prev_unk_expected[1] + 2:]\\\n",
    "                .index(tokens[unk_pointer + 1]) + 1\n",
    "            tid = unk_pointer\n",
    "            offsets_token = offsets_token[:tid] + [prev_unk_expected]\n",
    "            cid = prev_unk_expected[1] + 1\n",
    "        else:\n",
    "            start_pos = norm_context[cid:].index(cur_token)\n",
    "            if start_pos > 0 and tokens[tid - 1] == tokenizer.unk_token:\n",
    "                offsets_token[-1][1] += start_pos\n",
    "                cid += start_pos\n",
    "                start_pos = 0\n",
    "            assert start_pos == 0, f'{start_pos} != 0 (cur: {cur_token}'\n",
    "            offsets_token.append([cid, cid + len(cur_token) - 1])\n",
    "            cid += len(cur_token)\n",
    "            while cid < len(norm_context) and norm_context[cid] in whitespace:\n",
    "                offsets_token[-1][1] += 1\n",
    "                cid += 1\n",
    "        tid += 1\n",
    "    if tokens[-1] == tokenizer.unk_token:\n",
    "        offsets_token[-1][1] = len(norm_context) - 1\n",
    "    else:\n",
    "        assert cid == len(norm_context) == offsets_token[-1][1] + 1, \\\n",
    "            'Offsets do not include all characters'\n",
    "    assert len(offsets_token) == len(tokens), \\\n",
    "        'The numbers of tokens and offsets are different'\n",
    "\n",
    "    offsets_mapping = [(offsets_norm_context[start], offsets_norm_context[end])\n",
    "                       for start, end in offsets_token]\n",
    "    return [(0, 0)] + offsets_mapping+[(0, 0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テスト\n",
    "examples = datasetdict['train'][:3]\n",
    "tokenized_examples = tokenizer(\n",
    "            examples['question' if pad_on_right else \"context\"],\n",
    "            examples['context' if pad_on_right else \"question\"],\n",
    "            return_overflowing_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "# features = preprocess_function(datasetdict['train'][:3])\n",
    "# print(features)\n",
    "# len(features[\"input_ids\"])\n",
    "# for tokens in zip(tokenized_examples[\"input_ids\"]):\n",
    "#     print(tokens)\n",
    "#     print(type(tokens[0]))\n",
    "# tokenized_examples[\"input_ids\"]\n",
    "# tokenizer.sep_token_id\n",
    "# tokens.index(tokenizer.sep_token_id) + 1\n",
    "tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all data\n",
    "tokenized_train_dataset = datasetdict[\"train\"].map(preprocess_function, batched=True, remove_columns=datasetdict[\"train\"].column_names)\n",
    "tokenized_valid_dataset = datasetdict[\"validation\"].map(preprocess_function, batched=True, remove_columns=datasetdict[\"train\"].column_names)\n",
    "tokenized_train_dataset.set_format(type='torch')\n",
    "tokenized_valid_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = args['pretrained_model'].split(\"/\")[-1]\n",
    "note = args[\"note\"]\n",
    "data_collator = default_data_collator\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir= f\"./model/{model_name}_{note}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=args[\"lr\"],\n",
    "    per_device_train_batch_size= args[\"batch_size\"],\n",
    "    per_device_eval_batch_size= args[\"eval_batch_size\"],\n",
    "    num_train_epochs=args[\"epochs\"],\n",
    "    weight_decay=args[\"weight_decay\"],\n",
    "    push_to_hub=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"{model_name}_{note}\",\n",
    "    load_best_model_at_end = True,\n",
    "    eval_accumulation_steps = args[\"eval_accumulation_steps\"] #メモリ対策？\n",
    ")\n",
    "\n",
    "trainer =  Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(start_positions, end_positions, start_preds,\n",
    "                        end_preds):\n",
    "    start_overlap = np.maximum(start_positions, start_preds)\n",
    "    end_overlap = np.minimum(end_positions, end_preds)\n",
    "    overlap = np.maximum(end_overlap - start_overlap + 1, 0)\n",
    "    pred_token_count = np.maximum(end_preds - start_preds + 1, 0)\n",
    "    ground_token_count = np.maximum(end_positions - start_positions + 1, 0)\n",
    "\n",
    "\n",
    "    precision = torch.nan_to_num(overlap / pred_token_count, nan=0.)\n",
    "    recall = torch.nan_to_num(overlap / ground_token_count, nan=0.)\n",
    "    f1 = torch.nan_to_num(\n",
    "        2 * precision * recall / (precision + recall), nan=0.)\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "    \n",
    "def calculate_exact_match(start_positions, end_positions, start_preds,\n",
    "                            end_preds):\n",
    "    equal_start = (start_preds == start_positions)\n",
    "    equal_end = (end_preds == end_positions)\n",
    "    return (equal_start * equal_end).to('cpu').detach().numpy().astype(np.float32)\n",
    "\n",
    "def get_result(dataset = tokenized_valid_dataset):\n",
    "    results = trainer.predict(dataset)\n",
    "    start_preds = results.predictions[0]\n",
    "    end_preds = results.predictions[1]\n",
    "    start_positions = results.label_ids[0]\n",
    "    end_positions = results.label_ids[1]\n",
    "    start_pred= torch.from_numpy(start_preds).argmax(dim=-1).cpu().detach()\n",
    "    end_pred= torch.from_numpy(end_preds).argmax(dim=-1).cpu().detach()\n",
    "    start_position = torch.from_numpy(start_positions).cpu().detach()\n",
    "    end_position = torch.from_numpy(end_positions).cpu().detach()\n",
    "    return start_pred,end_pred,start_position,end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pred,end_pred,start_position,end_position = get_result(tokenized_valid_dataset)\n",
    "f1_metrics = calculate_f1_score(start_position,end_position,start_pred,end_pred)\n",
    "\n",
    "precision = f1_metrics['precision'].mean()\n",
    "recall = f1_metrics['recall'].mean()\n",
    "f1 = f1_metrics['f1'].mean()\n",
    "em = calculate_exact_match(start_position,end_position,start_pred,end_pred).mean()\n",
    "\n",
    "print(\"========== CV ==========\")\n",
    "print(\"f1:\",f1.item())\n",
    "print(\"em:\",em.item())\n",
    "\n",
    "wandb.log({\"f1\": f1, \"em\": em})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions =[\"サイバーエージェントはいつ創業したの？\",\"世の中にはどんな広告媒体があるの？\",\"どうやって広告代理事業を行なっているの？\",\"何を軸に事業を展開しているの？\",\"サイバーエージェントは何から始まっている会社なの？\"]\n",
    "questions =[\"高橋 是清の職業は?\",\"高橋 是清は立憲政友会の第何代総裁？\",\"高橋 是清はなんと知られているの？\",\"高橋 是清の愛称は？\",\"高橋 是清の幼名は？\"]\n",
    "#questions =[\"どんな環境下で挑戦できるの？\",\"最近はサーバサイドエンジニアも対象に何が行われているの？\",\"最近はサーバサイドエンジニアも対象に何が実施されているの？\",\"どんな組織風土があるの？\",\"データサイエンスコンペティションはどんな形式？\"]\n",
    "#context = 'インターネットを軸に様々な事業を展開しております。 まず、サイバーエージェントは1998年に創業し、今年16周年目という節目にあたります。元々はBtoBといわれる対企業向けのビジネスとして、インターネットの広告代理事業から始まっている会社です。創業当初から行っており今も主力事業となっているうちの一つがインターネットの広告代理店の事業です。世の中には様々な広告の媒体があると思います。テレビCMもあれば、屋外広告、電車、雑誌、新聞、ラジオなどがある中で、私たちはインターネット専業で広告代理事業を行っています。総合広告代理店はTVを中心に全て扱うと思うのですが、私たちはその中でインターネットをメインとした専業でやっています。ここがBtoBといわれる対企業向けのビジネスとしての事業内容です'\n",
    "context = '高橋 是清（たかはし これきよ、1854年9月19日〈嘉永7年閏7月27日〉 - 1936年〈昭和11年〉2月26日）は、明治から昭和にかけての日本の財政家、日銀総裁、政治家[1]。立憲政友会第4代総裁。第20代内閣総理大臣（在任: 1921年〈大正10年〉11月13日 - 1922年〈大正11年〉6月12日）。栄典は正二位大勲位子爵。幼名は和喜次（わきじ）。近代日本を代表する財政家として知られ、総理大臣としてよりも大蔵大臣としての評価の方が高い。愛称は「ダルマさん」。'\n",
    "#context ='継続して価値を提供するためにチームとしての力を最大化して開発しようという組織風土があります。最新の情報キャッチアップできるようGoogle I/OやAWS re:Invent、国際学会などの海外で開催されるカンファレンスへの参加制度、勉強会やゼミ制度などがあり、切磋琢磨できる環境があります。最近ではサーバサイドエンジニアも対象とした機械学習ハンズオンやKaggle形式のデータサイエンスコンペティションなども実施されています。日々最新技術やマーケット情報が飛び交う環境下で技術感度の高いエンジニアと一緒にこの大きな時代の転換期にこそできる挑戦をしませんか？'\n",
    "def predict_answer(question, context):\n",
    "    inputs = tokenizer(\n",
    "    question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    # Get the most likely beginning of answer with the argmax of the score.\n",
    "    answer_start = torch.argmax(answer_start_scores)-1\n",
    "\n",
    "    answer_end = torch.argmax(answer_end_scores) -1\n",
    "    # Get the most likely end of answer with the argmax of the score.\n",
    "    # 1 is added to `answer_end` because the index pointed by score is inclusive.\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(\"質問\",question)\n",
    "    print(\"answer\",answer)\n",
    "    print(\"answer_start\",answer_start)\n",
    "    print(\"answer_end\",answer_end)\n",
    "    \n",
    "for i in range(len(questions)):\n",
    "    question = questions[i]\n",
    "    predict_answer(question, context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most likely beginning of answer with the argmax of the score.\n",
    "answer_start = torch.argmax(answer_start_scores)-1\n",
    "\n",
    "answer_end = torch.argmax(answer_end_scores) -1\n",
    "# Get the most likely end of answer with the argmax of the score.\n",
    "# 1 is added to `answer_end` because the index pointed by score is inclusive.\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "print(answer)\n",
    "print(answer_start)\n",
    "print(answer_end)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
